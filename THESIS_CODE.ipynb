{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bdba9e65",
   "metadata": {},
   "source": [
    "# A Motif-based Perspective on Network Structue and it's Influence on Epidemic Behaviour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1140,
   "id": "f8d50c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.linalg as lg\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import EoN\n",
    "from EoN import Simulation_Investigation\n",
    "import scipy.special\n",
    "import pandas as pd\n",
    "import operator as op\n",
    "from scipy import stats\n",
    "from scipy.stats import linregress\n",
    "from scipy.stats import expon\n",
    "from itertools import islice\n",
    "from scipy.integrate import odeint\n",
    "from matplotlib.lines import Line2D\n",
    "from scipy.stats import shapiro\n",
    "%config InlineBackend.figure_format = 'svg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a77e05ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1000\n",
    "net = 180\n",
    "net_per_group = 30\n",
    "group = 6\n",
    "no_tot_sim = 5400\n",
    "gamma = 0.076\n",
    "tau = 0.06\n",
    "tmax = 365\n",
    "n = 1000\n",
    "no_sim = 30\n",
    "sim_p_group = 900\n",
    "ex_ep = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55752330",
   "metadata": {},
   "source": [
    "Note we refer to network groups, this we use to differentiate between networks that generated with the different $\\xi$ values and the control group of Barabasi Albert networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88029f5e",
   "metadata": {},
   "source": [
    "# Calulating the number of triangle-based motifs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c581f783",
   "metadata": {},
   "source": [
    "Create a list with every filename representing the name of each contact network we will use. BAmatrix represents Barabasi Albert networks, BAplusTN represents our modified networks. We include the number of the network and the tr value used to generate the network in MATLAB where tr = triangles "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e02ade3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = []\n",
    "for i in range(1,31):\n",
    "    BAmatrix = 'BAmatrix'+'_'+ str(i) +'.txt'\n",
    "    filenames.append(BAmatrix)\n",
    "for j in range(31,61):\n",
    "    BAplusTN = 'BAplusTN'+'_'+ str(j)+'_'+ str(500)+ '.txt'\n",
    "    filenames.append(BAplusTN)\n",
    "for k in range(61,91):\n",
    "    BAplusTN = 'BAplusTN'+'_'+ str(k)+'_'+str(750)+ '.txt'\n",
    "    filenames.append(BAplusTN)\n",
    "for l in range(91,121):\n",
    "    BAplusTN = 'BAplusTN'+'_'+ str(l)+'_'+str(1000)+'.txt'\n",
    "    filenames.append(BAplusTN)\n",
    "for p in range(121,151):\n",
    "    BAplusTN = 'BAplusTN'+'_'+ str(p)+'_'+ str(1250)+'.txt'\n",
    "    filenames.append(BAplusTN)\n",
    "for t in range(151,181):\n",
    "    BAplusTN = 'BAplusTN'+'_'+ str(t)+'_'+ str(1500)+ '.txt'\n",
    "    filenames.append(BAplusTN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c639d4b",
   "metadata": {},
   "source": [
    "Read csv files for each network, calculate the number of triangle-based motifs in each. Store in 4 lists for each motif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65e68cd1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Triangle_motif = []\n",
    "Diamond_motif = []\n",
    "Tadpole_motif = []\n",
    "Bowtie_motif = []\n",
    "for i in range(net):\n",
    "    with open(filenames[i], 'r') as file:\n",
    "        data_i = file.read()\n",
    "        dat_i = np.matrix(data_i)\n",
    "        adj_i = np.reshape(dat_i, (n, n))\n",
    "        \n",
    "        \n",
    "        TRI_motif = (1/6)*np.trace(adj_i**3)\n",
    "        Triangle_motif.append(TRI_motif)\n",
    "        \n",
    "        adj_i_2 = adj_i**2 \n",
    "        DI_motif = (1/4)*np.sum([(adj_i_2[k,j]*adj_i[k,j])*((adj_i_2[k,j]*adj_i[k,j])-1) for k in range(n) for j in range(n)])\n",
    "        Diamond_motif.append(DI_motif)\n",
    "\n",
    "        \n",
    "\n",
    "        adj_i_3 = adj_i**3  \n",
    "        s = adj_i.sum(axis = 1)       \n",
    "        TAD_motif = np.sum([(1/2)*(adj_i_3[k,k])*(int(s[k]) - 2) for k in range(n) if int(s[k]) >2])\n",
    "        Tadpole_motif.append(TAD_motif)\n",
    "        \n",
    "    \n",
    "        BOW_motif = (1/2)*(np.sum([(1/2)*(adj_i_3[k,k])*((1/2)*(adj_i_3[k,k])-(1)) for k in range(n)])) - 2*Diamond_motif[i]\n",
    "        Bowtie_motif.append(BOW_motif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96c504b",
   "metadata": {},
   "source": [
    "Store number of triangle based motifs per network in table form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 991,
   "id": "d89ae116",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'Triangle motif':  Triangle_motif ,\n",
    "        'Diamond motif': Diamond_motif,\n",
    "        'Tadpole motif': Tadpole_motif,\n",
    "        'Bowtie motif': Bowtie_motif}\n",
    "df = pd.DataFrame(data, index = filenames)\n",
    "df.to_csv('Number_of_motifs_per_network.csv', index=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5e3e8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "motifs = pd.read_csv('Number_of_motifs_per_network.csv')\n",
    "Triangle_motif = motifs[\"Triangle motif\"]\n",
    "Diamond_motif = motifs[\"Diamond motif\"]\n",
    "Tadpole_motif = motifs[\"Tadpole motif\"]\n",
    "Bowtie_motif = motifs[\"Bowtie motif\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1029eed",
   "metadata": {},
   "source": [
    "Now we will create a lists for each group with the average number of each motif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5202ae17",
   "metadata": {},
   "outputs": [],
   "source": [
    "tri = np.array_split(np.array(Triangle_motif), group)\n",
    "av_tri_1 = round(np.sum(tri[0])/net_per_group)\n",
    "av_tri_2 = round(np.sum(tri[1])/net_per_group)\n",
    "av_tri_3 = round(np.sum(tri[2])/net_per_group)\n",
    "av_tri_4 = round(np.sum(tri[3])/net_per_group)\n",
    "av_tri_5 = round(np.sum(tri[4])/net_per_group)\n",
    "av_tri_6 = round(np.sum(tri[5])/net_per_group)\n",
    "av_tri = [av_tri_1,av_tri_2,av_tri_3,av_tri_4,av_tri_5,av_tri_6]\n",
    "\n",
    "di = np.array_split(np.array(Diamond_motif), group)\n",
    "av_di_1 = round(np.sum(di[0])/net_per_group)\n",
    "av_di_2 = round(np.sum(di[1])/net_per_group)\n",
    "av_di_3 = round(np.sum(di[2])/net_per_group)\n",
    "av_di_4 = round(np.sum(di[3])/net_per_group)\n",
    "av_di_5 = round(np.sum(di[4])/net_per_group)\n",
    "av_di_6 = round(np.sum(di[5])/net_per_group)\n",
    "av_di = [av_di_1,av_di_2,av_di_3,av_di_4,av_di_5,av_di_6]\n",
    "\n",
    "tad = np.array_split(np.array(Tadpole_motif), group)\n",
    "av_tad_1 = round(np.sum(tad[0])/net_per_group)\n",
    "av_tad_2 = round(np.sum(tad[1])/net_per_group)\n",
    "av_tad_3 = round(np.sum(tad[2])/net_per_group)\n",
    "av_tad_4 = round(np.sum(tad[3])/net_per_group)\n",
    "av_tad_5 = round(np.sum(tad[4])/net_per_group)\n",
    "av_tad_6 = round(np.sum(tad[5])/net_per_group)\n",
    "av_tad = [av_tad_1,av_tad_2,av_tad_3,av_tad_4,av_tad_5,av_tad_6]\n",
    "\n",
    "\n",
    "bow = np.array_split(np.array(Bowtie_motif), group)\n",
    "av_bow_1 = round(np.sum(bow[0])/net_per_group)\n",
    "av_bow_2 = round(np.sum(bow[1])/net_per_group)\n",
    "av_bow_3 = round(np.sum(bow[2])/net_per_group)\n",
    "av_bow_4 = round(np.sum(bow[3])/net_per_group)\n",
    "av_bow_5 = round(np.sum(bow[4])/net_per_group)\n",
    "av_bow_6 = round(np.sum(bow[5])/net_per_group)\n",
    "av_bow = [av_bow_1,av_bow_2,av_bow_3,av_bow_4,av_bow_5,av_bow_6]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f265e8",
   "metadata": {},
   "source": [
    "Store average number of motifs per group into a table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 992,
   "id": "c1ef6209",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'Average # Triangles':  av_tri ,\n",
    "        'Average # Diamonds': av_di,\n",
    "        'Average # Tadpoles': av_tad,\n",
    "        'Average # Bowties': av_bow}\n",
    "index_titles = ['Group 1', 'Group 2', 'Group 3', 'Group 4', 'Group 5', 'Group 6']\n",
    "df2 = pd.DataFrame(data, index = index_titles )\n",
    "df2.to_csv('Average_#_of_motifs_per_network.csv', index=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee9c297",
   "metadata": {},
   "source": [
    "### Plot comparissons of motifs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b69857b",
   "metadata": {},
   "source": [
    "Triangle vs Diamond, Triangle vs Tadpole, Triangle vs Bowtie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2074,
   "id": "0de30931",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(Triangle_motif, Diamond_motif, color = \"black\")\n",
    "plt.title('Triangle motifs vs Diamond motifs')\n",
    "plt.ylabel('Number of diamond motifs')\n",
    "plt.xlabel('The number of triangle motifs')\n",
    "plt.xlim(xmin = 0)\n",
    "plt.ylim(ymin = 0)\n",
    "plt.savefig(\"Triangle_vs_Diamond\",dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "\n",
    "plt.scatter(Triangle_motif, Tadpole_motif, color = \"black\")\n",
    "plt.title('Triangle motifs vs Tadpole motifs')\n",
    "plt.ylabel('Number of tadpole motifs')\n",
    "plt.xlabel('Number of triangle motifs')\n",
    "plt.xlim(xmin = 0)\n",
    "plt.ylim(ymin = 0)\n",
    "plt.savefig(\"Triangle_vs_Tadpole\",dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "\n",
    "plt.scatter(Triangle_motif, Bowtie_motif, color = \"black\")\n",
    "plt.title('Triangle motifs vs Bowtie motifs')\n",
    "plt.ylabel('Number of bowtie motifs')\n",
    "plt.xlabel('Number of triangle motifs')\n",
    "plt.xlim(xmin = 0)\n",
    "plt.ylim(ymin = 0)\n",
    "plt.savefig(\"Triangle_vs_Bowtie\",dpi=300, bbox_inches='tight')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d21a199",
   "metadata": {},
   "source": [
    "Diamond vs Bowtie, Diamond vs Tadpole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2073,
   "id": "ccd878ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(Diamond_motif, Bowtie_motif,color = \"black\")\n",
    "plt.title('Diamond motifs vs Bowtie motifs')\n",
    "plt.ylabel('Number of bowtie motifs')\n",
    "plt.xlabel('Number of diamond motifs')\n",
    "plt.xlim(xmin = 0)\n",
    "plt.ylim(ymin = 0)\n",
    "plt.savefig(\"Diamond_vs_Bowtie\",dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "plt.scatter(Diamond_motif, Tadpole_motif,color = \"black\")\n",
    "plt.title('Diamond motifs vs Tadpole motifs')\n",
    "plt.ylabel('Number of tadpole motifs')\n",
    "plt.xlabel('Number of diamond motifs')\n",
    "plt.xlim(xmin = 0)\n",
    "plt.ylim(ymin = 0)\n",
    "plt.savefig(\"Diamond_vs_Tadpole\",dpi=300, bbox_inches='tight')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48bf06a",
   "metadata": {},
   "source": [
    "Bowtie vs Tadpole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2072,
   "id": "39744987",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(Bowtie_motif,Tadpole_motif,color = \"black\")\n",
    "plt.title('Bowtie motifs vs Tadpole Motifs')\n",
    "plt.ylabel('Number of  tadpole motifs')\n",
    "plt.xlabel('Number of  bowtie motifs')\n",
    "plt.xlim(xmin = 0)\n",
    "plt.ylim(ymin = 0)\n",
    "plt.savefig(\"Tadpole_vs_Bowtie\",dpi=300, bbox_inches='tight')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfec3f1b",
   "metadata": {},
   "source": [
    "# Simulation of SIR epidemics on networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe5ad8f",
   "metadata": {},
   "source": [
    "Create a list for of filenames for simulation results. 30 simulations per network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4440b03e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filenames2 = []\n",
    "for i in range(net):\n",
    "    for j in range(net_per_group):\n",
    "        name = 'Simulation_parameters'+str(i)+'_'+str(j)+'.csv'\n",
    "        filenames2.append(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "81d6376a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(net):\n",
    "    with open(filenames[i], 'r') as file:\n",
    "        data_i = file.read()\n",
    "        dat_i = np.matrix(data_i)\n",
    "        adj_i = np.reshape(dat_i, (n, n))\n",
    "        \n",
    "        for j in range(no_sim):\n",
    "            G = nx.from_numpy_matrix(adj_i)\n",
    "            t, S, I, R = EoN.fast_SIR(G, tau, gamma,\n",
    "                            initial_infecteds = None, rho = None)\n",
    "            \n",
    "            data = {'Time':  t ,\n",
    "                    'Susceptible': S,\n",
    "                    'Infected': I,\n",
    "                    'Recovered': R}\n",
    "            df = pd.DataFrame(data)\n",
    "            df[\"Days\"] = np.floor(df[\"Time\"])\n",
    "            df.to_csv('Simulation_parameters'+str(i)+'_'+str(j)+'.csv', index=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2faa2ce5",
   "metadata": {},
   "source": [
    "## Epidemic Size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b28261e",
   "metadata": {},
   "source": [
    " Calculate the epidemic size for each simulation:\n",
    " \n",
    " Epidemic size = $S_{Initial} - S_{Final} +1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2088,
   "id": "b20a39cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "Epidemic_size = []\n",
    "for i in range(no_tot_sim):\n",
    "        data = pd.read_csv(filenames2[i])\n",
    "        S_initial = list(data[\"Susceptible\"])[0]\n",
    "        S_final = list(data[\"Susceptible\"])[-1]\n",
    "        Epidemic = (S_initial - S_final)+1\n",
    "        Epidemic_size.append(Epidemic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1dc8b92",
   "metadata": {},
   "source": [
    "The epidemic size for simulations per network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2091,
   "id": "462197b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Epidemics = np.array_split(np.array(Epidemic_size), net)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2696291f",
   "metadata": {},
   "source": [
    "Separate results for epidemics that take off and that don't."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "56731763",
   "metadata": {},
   "outputs": [],
   "source": [
    "Epidemics_more_than_10 = []\n",
    "for i in range(net):\n",
    "    Epidemics_pnn=[]\n",
    "    for k in Epidemics[i]:\n",
    "        if k > ex_ep:\n",
    "            Epidemics_pnn.append(k)\n",
    "    Epidemics_more_than_10.append(Epidemics_pnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca32014",
   "metadata": {},
   "source": [
    "Average epidemics sizes > 10 per network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1ea3f200",
   "metadata": {},
   "outputs": [],
   "source": [
    "av_ep_size_ex_10_pn = []\n",
    "for i in range(net):\n",
    "    av_ep_ex_10 = np.sum(Epidemics_more_than_10[i])/len(Epidemics_more_than_10[i])\n",
    "    av_ep_size_ex_10_pn.append(av_ep_ex_10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7057876",
   "metadata": {},
   "source": [
    "Plots for the average epidemic size per network and number of triangle-based motifs for each network, excluding 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2131,
   "id": "d82761ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#plt.scatter(Triangle_motif,av_ep_size_pn)\n",
    "plt.scatter(Triangle_motif,av_ep_size_ex_10_pn,color = \"black\")\n",
    "plt.xlabel(\"Number of triangle motifs\")\n",
    "plt.ylabel(\"Average Epidemic Size\")\n",
    "plt.ylim(ymin = 0, ymax = 1000)\n",
    "plt.xlim(xmin = 0)\n",
    "plt.savefig(\"Triangle_motifs_vs_Average_ep_size_ex_10\",dpi=300, bbox_inches='tight')\n",
    "#plt.title(\"Number of Triangle motifs vs Average Epidemic Size per network ($>10$)\")\n",
    "plt.close()\n",
    "\n",
    "plt.scatter(Diamond_motif,av_ep_size_ex_10_pn,color = \"black\")\n",
    "plt.xlabel(\"Number of diamond motifs\")\n",
    "plt.ylabel(\"Average Epidemic Size\")\n",
    "plt.ylim(ymin = 0, ymax = 1000)\n",
    "plt.xlim(xmin = 0)\n",
    "plt.savefig(\"Diamond_motifs_vs_Average_ep_size_ex_10\")\n",
    "#plt.title(\"Number of Diamond motifs vs Average Epidemic Size per network ($>10$)\")\n",
    "plt.close()\n",
    "\n",
    "plt.scatter(Tadpole_motif,av_ep_size_ex_10_pn,color = \"black\")\n",
    "plt.xlabel(\"Number of tadpole motifs\")\n",
    "plt.ylabel(\"Average Epidemic Size\")\n",
    "plt.ylim(ymin = 0, ymax = 1000)\n",
    "plt.xlim(xmin = 0)\n",
    "plt.savefig(\"Tadpole_motifs_vs_Average_ep_size_ex_10\")\n",
    "plt.title(\"Number of Tadpole motifs vs Average Epidemic Size per network ($>10$)\")\n",
    "plt.close()\n",
    "\n",
    "plt.scatter(Bowtie_motif,av_ep_size_ex_10_pn,color = \"black\")\n",
    "plt.xlabel(\"Number of bowtie motifs\")\n",
    "plt.ylabel(\"Average Epidemic Size\")\n",
    "plt.ylim(ymin = 0, ymax = 1000)\n",
    "plt.xlim(xmin = 0)\n",
    "plt.savefig(\"Bowtie_motifs_vs_Average_ep_size_ex_10\")\n",
    "plt.title(\"Number of Bowtie motifs vs Average Epidemic Size per network ($>10$)\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51eef7db",
   "metadata": {},
   "source": [
    "Store the data in a table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2092,
   "id": "e0e77f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Network = list(range(1,net+1))\n",
    "data = {'Average Epidemic size per network(excl. 10)': av_ep_size_ex_10_pn,\n",
    "        'Triangle motif':  Triangle_motif ,\n",
    "        'Diamond motif': Diamond_motif,\n",
    "        'Tadpole motif': Tadpole_motif,\n",
    "        'Bowtie motif': Bowtie_motif}\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv('Average_epidemic_size_pn_vs_motifs.csv', index = Network) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08dcd5fa",
   "metadata": {},
   "source": [
    "Create tables for average epidemic size for control networks and networks with the same $\\xi$ value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89c927b",
   "metadata": {},
   "source": [
    "Average epidemic size excluding sizes below and equal to 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dea7b162",
   "metadata": {},
   "outputs": [],
   "source": [
    "Av_Epidemic_size_ex_10_pg = list(np.array_split(np.array(av_ep_size_ex_10_pn), group)) \n",
    "Av_Epidemic_size_ex_10_G1 =list(Av_Epidemic_size_ex_10_pg[0])\n",
    "Av_Epidemic_size_ex_10_G2 =list(Av_Epidemic_size_ex_10_pg[1])\n",
    "Av_Epidemic_size_ex_10_G3 =list(Av_Epidemic_size_ex_10_pg[2])\n",
    "Av_Epidemic_size_ex_10_G4 =list(Av_Epidemic_size_ex_10_pg[3])\n",
    "Av_Epidemic_size_ex_10_G5 =list(Av_Epidemic_size_ex_10_pg[4])\n",
    "Av_Epidemic_size_ex_10_G6 =list(Av_Epidemic_size_ex_10_pg[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0d410240",
   "metadata": {},
   "outputs": [],
   "source": [
    "Triangle_motif_pg = list(np.array_split(np.array(Triangle_motif), group)) \n",
    "Triangle_motif_G1 = list(Triangle_motif_pg[0])\n",
    "Triangle_motif_G2 = list(Triangle_motif_pg[1])\n",
    "Triangle_motif_G3 = list(Triangle_motif_pg[2])\n",
    "Triangle_motif_G4 = list(Triangle_motif_pg[3])\n",
    "Triangle_motif_G5 = list(Triangle_motif_pg[4])\n",
    "Triangle_motif_G6 = list(Triangle_motif_pg[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2258fdbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Diamond_motif_pg = list(np.array_split(np.array(Diamond_motif), group))\n",
    "Diamond_motif_G1 = list(Diamond_motif_pg[0])\n",
    "Diamond_motif_G2 = list(Diamond_motif_pg[1])\n",
    "Diamond_motif_G3 = list(Diamond_motif_pg[2])\n",
    "Diamond_motif_G4 = list(Diamond_motif_pg[3])\n",
    "Diamond_motif_G5 = list(Diamond_motif_pg[4])\n",
    "Diamond_motif_G6 = list(Diamond_motif_pg[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d086a26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Tadpole_motif_pg = list(np.array_split(np.array(Tadpole_motif), group)) \n",
    "Tadpole_motif_G1 = list(Tadpole_motif_pg[0])\n",
    "Tadpole_motif_G2 = list(Tadpole_motif_pg[1])\n",
    "Tadpole_motif_G3 = list(Tadpole_motif_pg[2])\n",
    "Tadpole_motif_G4 = list(Tadpole_motif_pg[3])\n",
    "Tadpole_motif_G5 = list(Tadpole_motif_pg[4])\n",
    "Tadpole_motif_G6 = list(Tadpole_motif_pg[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ae981dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "Bowtie_motif_pg = list(np.array_split(np.array(Bowtie_motif), group)) \n",
    "Bowtie_motif_G1 = list(Bowtie_motif_pg[0])\n",
    "Bowtie_motif_G2 = list(Bowtie_motif_pg[1])\n",
    "Bowtie_motif_G3 = list(Bowtie_motif_pg[2])\n",
    "Bowtie_motif_G4 = list(Bowtie_motif_pg[3])\n",
    "Bowtie_motif_G5 = list(Bowtie_motif_pg[4])\n",
    "Bowtie_motif_G6 = list(Bowtie_motif_pg[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "51ab7f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = list(range(1,net_per_group+1))\n",
    "data1 = {'Average Epidemic Size ':  Av_Epidemic_size_G1  ,\n",
    "         'Average Epidemic size per network(excl. 10)': Av_Epidemic_size_ex_10_G1,\n",
    "        'Triangle motif':  Triangle_motif_G1,\n",
    "        'Diamond motif': Diamond_motif_G1,\n",
    "        'Tadpole motif': Tadpole_motif_G1,\n",
    "        'Bowtie motif': Bowtie_motif_G1}\n",
    "df1 = pd.DataFrame(data1)\n",
    "df1.to_csv('Average_epidemic_size_G1_vs_motifs.csv', index = ind) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0ae1ccf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = list(range(1,net_per_group+1))\n",
    "data2 = {'Average Epidemic Size ':  Av_Epidemic_size_G2  ,\n",
    "         'Average Epidemic size per network(excl. 10)': Av_Epidemic_size_ex_10_G2,\n",
    "        'Triangle motif':  Triangle_motif_G2,\n",
    "        'Diamond motif': Diamond_motif_G2,\n",
    "        'Tadpole motif': Tadpole_motif_G2,\n",
    "        'Bowtie motif': Bowtie_motif_G2}\n",
    "df2 = pd.DataFrame(data2)\n",
    "df2.to_csv('Average_epidemic_size_G2_vs_motifs.csv', index = ind) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "41d189cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = list(range(1,net_per_group+1))\n",
    "data3 = {'Average Epidemic Size ':  Av_Epidemic_size_G3 ,\n",
    "         'Average Epidemic size per network(excl. 10)': Av_Epidemic_size_ex_10_G3,\n",
    "        'Triangle motif':  Triangle_motif_G3,\n",
    "        'Diamond motif': Diamond_motif_G3,\n",
    "        'Tadpole motif': Tadpole_motif_G3,\n",
    "        'Bowtie motif': Bowtie_motif_G3}\n",
    "df3 = pd.DataFrame(data3)\n",
    "df3.to_csv('Average_epidemic_size_G3_vs_motifs.csv', index = ind) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9cfd0b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = list(range(1,net_per_group+1))\n",
    "data4 = {'Average Epidemic Size ':  Av_Epidemic_size_G4,\n",
    "         'Average Epidemic size per network(excl. 10)': Av_Epidemic_size_ex_10_G4,\n",
    "        'Triangle motif':  Triangle_motif_G4,\n",
    "        'Diamond motif': Diamond_motif_G4,\n",
    "        'Tadpole motif': Tadpole_motif_G4,\n",
    "        'Bowtie motif': Bowtie_motif_G4}\n",
    "df4 = pd.DataFrame(data4)\n",
    "df4.to_csv('Average_epidemic_size_G4_vs_motifs.csv', index = ind) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bb9227ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = list(range(1,net_per_group+1))\n",
    "data5 = {'Average Epidemic Size ':  Av_Epidemic_size_G5,\n",
    "         'Average Epidemic size per network(excl. 10)': Av_Epidemic_size_ex_10_G5,\n",
    "        'Triangle motif':  Triangle_motif_G5,\n",
    "        'Diamond motif': Diamond_motif_G5,\n",
    "        'Tadpole motif': Tadpole_motif_G5,\n",
    "        'Bowtie motif': Bowtie_motif_G5}\n",
    "df5 = pd.DataFrame(data5)\n",
    "df5.to_csv('Average_epidemic_size_G5_vs_motifs.csv', index = ind) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5afb7588",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = list(range(1,net_per_group+1))\n",
    "data6 = {'Average Epidemic Size ':  Av_Epidemic_size_G6,\n",
    "         'Average Epidemic size per network(excl. 10)': Av_Epidemic_size_ex_10_G6,\n",
    "        'Triangle motif':  Triangle_motif_G6,\n",
    "        'Diamond motif': Diamond_motif_G6,\n",
    "        'Tadpole motif': Tadpole_motif_G6,\n",
    "        'Bowtie motif': Bowtie_motif_G6}\n",
    "df6 = pd.DataFrame(data6)\n",
    "df6.to_csv('Average_epidemic_size_G6_vs_motifs.csv', index = ind)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa55d688",
   "metadata": {},
   "source": [
    "Calculate variance, std and percentiles for epidemic size per network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc85a4d",
   "metadata": {},
   "source": [
    "For all epidemics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d3e7446d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Epidemic_size_pn =list(np.array_split(np.array(Epidemic_size), net)) \n",
    "epsize_variance = []\n",
    "percentile_95 = []\n",
    "percentile_5 =[]\n",
    "std = []\n",
    "for i in range(net):\n",
    "    var = np.var(Epidemic_size_pn[i])\n",
    "    epsize_variance.append(var)\n",
    "    per95 = np.percentile(Epidemic_size_pn[i],95)\n",
    "    percentile_95.append(per95)\n",
    "    per5 = np.percentile(Epidemic_size_pn[i],5)\n",
    "    percentile_5.append(per5)\n",
    "    sd = np.std(Epidemic_size_pn[i], ddof=1)/np.sqrt(len(Epidemic_size_pn[i]))\n",
    "    std.append(sd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f366a3",
   "metadata": {},
   "source": [
    "For epidemics that took hold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e884a412",
   "metadata": {},
   "outputs": [],
   "source": [
    "epsize_variance_ex_10= []\n",
    "percentile_95_ex_10 = []\n",
    "percentile_5_ex_10 =[]\n",
    "std_ex_10 = []\n",
    "for i in range(net):\n",
    "    var1 = np.var(Epidemics_more_than_10[i])\n",
    "    epsize_variance_ex_10.append(var1)\n",
    "    per951 = np.percentile(Epidemics_more_than_10[i] , 95)\n",
    "    percentile_95_ex_10.append(per951)\n",
    "    per51 = np.percentile(Epidemics_more_than_10[i], 5)\n",
    "    percentile_5_ex_10.append(per51)\n",
    "    sd1 = np.std(Epidemics_more_than_10[i])/np.sqrt(len(Epidemics_more_than_10[i]))\n",
    "    std_ex_10.append(sd1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1522,
   "id": "152996ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_range_ep_size = [(min(x),max(x)) for x in Epidemics_more_than_10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1534,
   "id": "b0b66165",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_range_ep_size_pg = list(np.array_split(np.array(max_range_ep_size), group))\n",
    "#max_range_ep_size_pg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9ba321",
   "metadata": {},
   "source": [
    "Save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d68f521c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = list(range(1,net+1))\n",
    "data8 = {'Average Epidemic Size per network': av_ep_size_pn,\n",
    "        'Variance':  epsize_variance,\n",
    "        '5th percentile': percentile_5,\n",
    "        '95th percentile': percentile_95,\n",
    "        'Standard deviation': std}\n",
    "df8 = pd.DataFrame(data8)\n",
    "df8.to_csv('Average_Epidemic_size_vs_variance.csv', index = ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a5a2c411",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = list(range(1,net+1))\n",
    "data8 = {'Average Epidemic Size per network excl. 1 or 2': av_ep_size_ex_10_pn,\n",
    "        'Variance':  epsize_variance_ex_10,\n",
    "        '5th percentile': percentile_5_ex_10,\n",
    "        '95th percentile': percentile_95_ex_10,\n",
    "        'Standard deviation': std_ex_10}\n",
    "df8 = pd.DataFrame(data8)\n",
    "df8.to_csv('Average_Epidemic_size_ex_1_2_vs_variance.csv', index = ind)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dafc0750",
   "metadata": {},
   "source": [
    "Plot average epidemic size > 10 for for networks vs motifs, including percentiles "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fc623fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "yerr01 = [(np.array(av_ep_size_ex_10_pn) - np.array(percentile_5_ex_10)),(np.array(percentile_95_ex_10) - np.array(av_ep_size_ex_10_pn))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2132,
   "id": "38377f01",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.errorbar(Triangle_motif, av_ep_size_ex_10_pn, yerr = yerr01 , markersize = 4, fmt = 'o',capsize=3,color='black',\n",
    "             ecolor='gray', elinewidth=1)\n",
    "plt.ylabel(\"Average Epidemic Size\")\n",
    "plt.xlabel(\"Number of triangle motifs\")\n",
    "plt.ylim(ymin = 0, ymax = 1000)\n",
    "plt.xlim(xmin = 0)\n",
    "#plt.yscale(\"log\")\n",
    "plt.savefig(\"Triangle_motifs_vs_Average_ep_size_ex_10_PERCENTILES\", dpi = 300, bbox_inches='tight')\n",
    "plt.title(\"Average epidemic size per network ($>10$)\")\n",
    "plt.close()\n",
    "\n",
    "plt.errorbar(Diamond_motif, av_ep_size_ex_10_pn, yerr = yerr01 , markersize = 4, fmt = 'o',capsize=3,color='black',\n",
    "             ecolor='gray', elinewidth=1)\n",
    "#plt.plot(Diamond_motif, slope*(Diamond_motif)+intercept, '--k'  )\n",
    "plt.ylabel(\"Average Epidemic Size\")\n",
    "plt.xlabel(\"Number of diamond motifs\")\n",
    "plt.ylim(ymin = 0, ymax = 1000)\n",
    "plt.xlim(xmin = 0)\n",
    "#plt.yscale(\"log\")\n",
    "plt.savefig(\"Diamond_motifs_vs_Average_ep_size_ex_10_PERCENTILES\",dpi=300, bbox_inches='tight')\n",
    "plt.title(\"The number of diamond motifs vs average epidemic size per network ($>10$)\")\n",
    "plt.close()\n",
    "\n",
    "plt.errorbar(Tadpole_motif, av_ep_size_ex_10_pn, yerr = yerr01 , markersize = 4, fmt = 'o',capsize=3,color='black',\n",
    "             ecolor='gray', elinewidth=1)\n",
    "plt.ylabel(\"Average Epidemic Size\")\n",
    "plt.xlabel(\"Number of tadpole motifs\")\n",
    "plt.ylim(ymin = 0, ymax = 1000)\n",
    "plt.xlim(xmin = 0)\n",
    "#plt.yscale(\"log\")\n",
    "plt.savefig(\"Tadpole_motifs_vs_Average_ep_size_ex_10_PERCENTILES\",dpi=300, bbox_inches='tight')\n",
    "plt.title(\"Average epidemic Size per network ($>10$)\")\n",
    "plt.close()\n",
    "\n",
    "plt.errorbar(Bowtie_motif, av_ep_size_ex_10_pn, yerr = yerr01 , markersize = 4, fmt = 'o',capsize=3,color='black',\n",
    "             ecolor='gray', elinewidth=1)\n",
    "plt.ylabel(\"Average Epidemic Size\")\n",
    "plt.xlabel(\"Number of bowtie motifs\")\n",
    "plt.ylim(ymin = 0, ymax = 1000)\n",
    "plt.xlim(xmin = 0)\n",
    "#plt.yscale(\"log\")\n",
    "plt.savefig(\"Bowtie_motifs_vs_Average_ep_size_ex_10_PERCENTILES\",dpi=300, bbox_inches='tight')\n",
    "plt.title(\"Average epidemic size per network ($>10$)\")\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a908a58",
   "metadata": {},
   "source": [
    "Testing correlation coefficients and significance for the average epidemic size per network and the number of trinagle-based motifs per network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 994,
   "id": "924d8178",
   "metadata": {},
   "outputs": [],
   "source": [
    "slope1, intercept1, r1, p1, se = linregress(Triangle_motif, av_ep_size_ex_10_pn)\n",
    "slope2, intercept3, r2, p2, se = linregress(Diamond_motif, av_ep_size_ex_10_pn)\n",
    "slope3, intercept3, r3, p3, se = linregress(Tadpole_motif, av_ep_size_ex_10_pn)\n",
    "slope4, intercept4, r4, p4, se = linregress(Bowtie_motif, av_ep_size_ex_10_pn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1549,
   "id": "dc3f28fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#r1,r2,r3,r4,p1,p2,p3,p4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa95f0c",
   "metadata": {},
   "source": [
    "## Average peak incidence per network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6915ed21",
   "metadata": {},
   "source": [
    "Create files for daily incidence and rolling means for each simulation. Found the number of infections per time step and grouped by day to sum infections per day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1628,
   "id": "28848cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(no_tot_sim):\n",
    "    daily_incidence = []\n",
    "    mean_rolling_incidence = []\n",
    "    sample = pd.read_csv(filenames2[j])\n",
    "    Days = list(sample[\"Days\"])\n",
    "    Recover_pts = list(sample[\"Recovered\"].diff(periods = 1))\n",
    "    incidence = (-1)*np.array(sample[\"Susceptible\"].diff(periods = 1))\n",
    "    \n",
    "    data = {\"Incidence\": incidence,\"Days\": list(sample[\"Days\"])}\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    d_i = df.groupby(\"Days\", as_index=False)[\"Incidence\"].sum()\n",
    "    \n",
    "    d_i[\"Days\"] = d_i[\"Days\"].astype(np.int64)\n",
    "\n",
    "    d_i = d_i.set_index(\"Days\")\n",
    "    d_i = d_i.reindex(range(tmax), fill_value = 0)\n",
    "    d_i = d_i.reset_index()\n",
    "\n",
    "    d_i[\"Rolling mean 3\"] = d_i[\"Incidence\"].rolling(3, center=True, closed = 'both').mean()\n",
    "    d_i[\"Rolling mean 7\"] = d_i[\"Incidence\"].rolling(7, center=True, closed = 'both').mean()\n",
    "    \n",
    "    d_i.to_csv('Incidence'+str(j)+'.csv', index = list(range(tmax)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1629,
   "id": "26adf27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames3 = []\n",
    "for i in range(no_tot_sim):\n",
    "    name = 'Incidence'+str(i)+'.csv'\n",
    "    filenames3.append(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d18dbe4",
   "metadata": {},
   "source": [
    "Excluded epidemics that did not take hold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1630,
   "id": "42893927",
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames_no_epidemic = []\n",
    "index_no_ep = []\n",
    "for y in [i for i, x in enumerate(Epidemic_size) if x <= ex_ep]:\n",
    "    t = 'Incidence'+str(y)+'.csv'\n",
    "    index_no_ep.append(y)\n",
    "    filenames_no_epidemic.append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1631,
   "id": "ebccf685",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1798"
      ]
     },
     "execution_count": 1631,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filenames_no_epidemic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2adde59",
   "metadata": {},
   "source": [
    "Peak_incidence and time for peak incidence for epidemics that took hold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1632,
   "id": "b7d17062",
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames3_1 = [x for x in filenames3 if x not in filenames_no_epidemic]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1735,
   "id": "cddc1a48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "range(0, 3602)"
      ]
     },
     "execution_count": 1735,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "range(len(filenames3_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1737,
   "id": "395a8915",
   "metadata": {},
   "outputs": [],
   "source": [
    "peak_incidence = []\n",
    "time_peak_incidence_1= []\n",
    "\n",
    "for k in range(len(filenames3_1)):\n",
    "    mysample = pd.read_csv(filenames3_1[k])\n",
    "    t = mysample[\"Rolling mean 7\"]\n",
    "    max_t = np.max(t)\n",
    "    time_p_incidence =  mysample[mysample[\"Rolling mean 7\"] == max_t].index.values\n",
    "    \n",
    "    peak_incidence.append(max_t)\n",
    "    time_peak_incidence_1.append(time_p_incidence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1701,
   "id": "6d677a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_peak_incidence = []\n",
    "for i in time_peak_incidence_1:\n",
    "    if len(i)>1:\n",
    "        time_peak_incidence.append(i[0])\n",
    "    else:\n",
    "        time_peak_incidence.append(int(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1950,
   "id": "91445b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "peak_incidence_rolling_mean_3 = []\n",
    "time_peak_incidence_1_rolling_mean_3= []\n",
    "\n",
    "for k in range(len(filenames3_1)):\n",
    "    mysample = pd.read_csv(filenames3_1[k])\n",
    "    t = mysample[\"Rolling mean 3\"]\n",
    "    max_t = np.max(t)\n",
    "    time_p_incidence =  mysample[mysample[\"Rolling mean 3\"] == max_t].index.values\n",
    "    \n",
    "    peak_incidence_rolling_mean_3.append(max_t)\n",
    "    time_peak_incidence_1_rolling_mean_3.append(time_p_incidence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1951,
   "id": "f0c2d53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_peak_incidence_rolling_mean_3 = []\n",
    "for i in time_peak_incidence_1_rolling_mean_3:\n",
    "    if len(i)>1:\n",
    "        time_peak_incidence_rolling_mean_3.append(i[0])\n",
    "    else:\n",
    "        time_peak_incidence_rolling_mean_3.append(int(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1925,
   "id": "6fa5871a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 1925,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shapiro(peak_incidence)[1] > 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1926,
   "id": "97b81d9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0, 64.75)"
      ]
     },
     "execution_count": 1926,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(peak_incidence), max(peak_incidence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274353bd",
   "metadata": {},
   "source": [
    "Plots for daily incidence per epidemics that took hold vs time for networks with the same $\\xi$ value and control networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1662,
   "id": "17f43063",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [len(x) for x in Epidemics_more_than_10] #number of epidemics that took hold per network\n",
    "y_pg = list(np.array_split(np.array(y), group)) # split the number of epidemics that took hold pn to pg\n",
    "yy_pg = [np.sum(x) for x in y_pg] # total number of epidemics that took hold per group\n",
    "filenames3_1_pg =  [list(islice(iter(filenames3_1), elem)) for elem in yy_pg] # split filenames into filenames pg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1636,
   "id": "fdfaa625",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 624, 1258, 1908, 2496, 3083, 3602])"
      ]
     },
     "execution_count": 1636,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myrange = np.cumsum(yy_pg) #range for epidemics that took hold per group\n",
    "myrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2032,
   "id": "3b0e59b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "currentNsmax_max = []\n",
    "index_max = []\n",
    "\n",
    "df = []\n",
    "\n",
    "\n",
    "\n",
    "for j in range(myrange[0]):\n",
    "    sample = pd.read_csv(filenames3_1[j])\n",
    "    df1 = sample[\"Incidence\"]\n",
    "    df.append(df1)\n",
    "    \n",
    "    currentNsmax = max(df1)\n",
    "    lines = Line2D(list(range(len(df1))), df1)\n",
    "    ax.add_line(lines)\n",
    "    \n",
    "    currentNsmax_max.append(currentNsmax)\n",
    "    \n",
    "\n",
    "combinded = pd.concat(df,axis=1)\n",
    "combinded.to_csv('Incidence_vs_time_bara.csv', index = list(range(tmax)))\n",
    "\n",
    "ax.set_xlim(0.0, 200)\n",
    "ax.set_ylim(0.0, max(currentNsmax_max))\n",
    "plt.ylabel(\"Daily Incidence\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.savefig(\"Graph_Incidence_vs_time_barab\",dpi=300, bbox_inches='tight' )\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2057,
   "id": "fd1095b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df11 = combinded.median(axis = 1)\n",
    "df11_2_5 = combinded.quantile(q= 0.025, axis=1)\n",
    "df11_25 = combinded.quantile(q= 0.25, axis=1)\n",
    "df11_75 = combinded.quantile(q= 0.75, axis=1)\n",
    "df11_97_5 = combinded.quantile(q= 0.975, axis=1)\n",
    "\n",
    "plt.plot(range(len(df11)),list(df11), label= \"Median\")\n",
    "plt.fill_between(range(len(df11)), np.array(df11)- (np.array(df11) - np.array(df11_25)), np.array(df11)+(np.array(df11_75) - np.array(df11)), alpha=0.2,edgecolor='#1B2ACC',\n",
    " facecolor='#089FFF', label =  r'$50\\%$'\" interval\")\n",
    "plt.fill_between(range(len(df11)), np.array(df11)- (np.array(df11) - np.array(df11_2_5)), np.array(df11)+(np.array(df11_97_5) - np.array(df11)), alpha =0.2, label = r'$95\\%$'\" interval\", edgecolor=(0,0,0,.4))\n",
    "plt.ylabel(\"Daily Incidence\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Time\")\n",
    "plt.xlim(xmin = 0, xmax = 100)\n",
    "plt.ylim(ymin = 0, ymax = 80)\n",
    "plt.title(\"Control\")\n",
    "plt.savefig(\"Daily incidence vs time bara\",dpi=300, bbox_inches='tight')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2037,
   "id": "78af3df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "currentNsmax_max = []\n",
    "index_max = []\n",
    "\n",
    "df2 = []\n",
    "for j in range(myrange[0],myrange[1]):\n",
    "    sample = pd.read_csv(filenames3_1[j])\n",
    "    df1 = sample[\"Incidence\"]\n",
    "    df2.append(df1)\n",
    "\n",
    "    currentNsmax = max(df1)\n",
    "    lines = Line2D(list(range(len(df1))), df1)\n",
    "    ax.add_line(lines)\n",
    "    \n",
    "    currentNsmax_max.append(currentNsmax)\n",
    "    \n",
    "combinded2 = pd.concat(df2,axis=1)\n",
    "combinded2.to_csv('Incidence_vs_time_250.csv', index = list(range(tmax)))\n",
    "\n",
    "ax.set_xlim(0.0, 200)\n",
    "ax.set_ylim(0.0, max(currentNsmax_max))\n",
    "plt.ylabel(\"Daily Incidence\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.savefig(\"Graph_Incidence_vs_time_500\",dpi=300, bbox_inches='tight' )\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2056,
   "id": "91364bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df22 = combinded2.median(axis = 1)\n",
    "df22_2_5 = combinded2.quantile(q= 0.025, axis=1)\n",
    "df22_25 = combinded2.quantile(q= 0.25, axis=1)\n",
    "df22_75 = combinded2.quantile(q= 0.75, axis=1)\n",
    "df22_97_5 = combinded2.quantile(q= 0.975, axis=1)\n",
    "\n",
    "plt.plot(range(len(df22)),list(df22), label = 'Median')\n",
    "plt.fill_between(range(len(df22)), np.array(df22)- (np.array(df22) - np.array(df22_25)), np.array(df22)+(np.array(df22_75) - np.array(df22)), alpha=0.2,edgecolor='#1B2ACC',\n",
    " facecolor='#089FFF', label = r'$50\\%$'\" interval\")\n",
    "plt.fill_between(range(len(df22)), np.array(df22)- (np.array(df22) - np.array(df22_2_5)), np.array(df22)+(np.array(df22_97_5) - np.array(df22)), alpha =0.2, label = r'$95\\%$'\" interval\", edgecolor=(0,0,0,.4))\n",
    "plt.ylabel(\"Daily Incidence\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Time\")\n",
    "plt.xlim(xmin = 0, xmax = 100)\n",
    "plt.ylim(ymin = 0, ymax = 80)\n",
    "plt.title(r'$\\xi = 500$')\n",
    "plt.savefig(\"Daily incidence vs time five\",dpi=300, bbox_inches='tight')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2039,
   "id": "1ec44d12",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "currentNsmax_max = []\n",
    "index_max = []\n",
    "\n",
    "df3 = []\n",
    "# array([ 624, 1258, 1908, 2496, 3083, 3602])\n",
    "  \n",
    "    \n",
    "for j in range(myrange[1],myrange[2]):\n",
    "    sample = pd.read_csv(filenames3_1[j])\n",
    "    df1 = sample[\"Incidence\"]\n",
    "    df3.append(df1)\n",
    "\n",
    "    currentNsmax = max(df1)\n",
    "    lines = Line2D(list(range(len(df1))), df1)\n",
    "    ax.add_line(lines)\n",
    "    \n",
    "    currentNsmax_max.append(currentNsmax)\n",
    "    \n",
    "#combinded3 = pd.concat(df3,axis=1)\n",
    "#combinded3.to_csv('Incidence_vs_time_750.csv', index = list(range(tmax)))\n",
    "\n",
    "ax.set_xlim(0.0, 200)\n",
    "ax.set_ylim(0.0, max(currentNsmax_max))\n",
    "plt.ylabel(\"Daily Incidence\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.savefig(\"Graph_Incidence_vs_time_750\",dpi=300, bbox_inches='tight' )\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2027,
   "id": "954b5fd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 624, 1258, 1908, 2496, 3083, 3602])"
      ]
     },
     "execution_count": 2027,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2070,
   "id": "e7dda884",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2070]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m fig \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39mfigure()\n\u001b[1;32m      7\u001b[0m plt\u001b[38;5;241m.\u001b[39mion()\n\u001b[0;32m----> 8\u001b[0m ax \u001b[38;5;241m=\u001b[39m \u001b[43mfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_subplot\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m111\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m currentNsmax_max \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     10\u001b[0m index_max \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/matplotlib/figure.py:772\u001b[0m, in \u001b[0;36mFigureBase.add_subplot\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    769\u001b[0m         args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mstr\u001b[39m(args[\u001b[38;5;241m0\u001b[39m])))\n\u001b[1;32m    770\u001b[0m     projection_class, pkw \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_projection_requirements(\n\u001b[1;32m    771\u001b[0m         \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 772\u001b[0m     ax \u001b[38;5;241m=\u001b[39m \u001b[43msubplot_class_factory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprojection_class\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    773\u001b[0m     key \u001b[38;5;241m=\u001b[39m (projection_class, pkw)\n\u001b[1;32m    774\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_add_axes_internal(ax, key)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/matplotlib/axes/_subplots.py:34\u001b[0m, in \u001b[0;36mSubplotBase.__init__\u001b[0;34m(self, fig, *args, **kwargs)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;124;03m----------\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;124;03m    Keyword arguments are passed to the Axes (sub)class constructor.\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# _axes_class is set in the subplot_class_factory\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_axes_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# This will also update the axes position.\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_subplotspec(SubplotSpec\u001b[38;5;241m.\u001b[39m_from_subplot_args(fig, args))\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/matplotlib/_api/deprecation.py:456\u001b[0m, in \u001b[0;36mmake_keyword_only.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m name_idx:\n\u001b[1;32m    451\u001b[0m     warn_deprecated(\n\u001b[1;32m    452\u001b[0m         since, message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassing the \u001b[39m\u001b[38;5;132;01m%(name)s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%(obj_type)s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    453\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpositionally is deprecated since Matplotlib \u001b[39m\u001b[38;5;132;01m%(since)s\u001b[39;00m\u001b[38;5;124m; the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    454\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter will become keyword-only \u001b[39m\u001b[38;5;132;01m%(removal)s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    455\u001b[0m         name\u001b[38;5;241m=\u001b[39mname, obj_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/matplotlib/axes/_base.py:632\u001b[0m, in \u001b[0;36m_AxesBase.__init__\u001b[0;34m(self, fig, rect, facecolor, frameon, sharex, sharey, label, xscale, yscale, box_aspect, **kwargs)\u001b[0m\n\u001b[1;32m    629\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_axisbelow(mpl\u001b[38;5;241m.\u001b[39mrcParams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maxes.axisbelow\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rasterization_zorder \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 632\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcla\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[38;5;66;03m# funcs used to format x and y - fall back on major formatters\u001b[39;00m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfmt_xdata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/matplotlib/axes/_base.py:1249\u001b[0m, in \u001b[0;36m_AxesBase.cla\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1246\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlegend_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1247\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontainers \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m-> 1249\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Disable grid on init to use rcParameter\u001b[39;00m\n\u001b[1;32m   1250\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrid(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gridOn, which\u001b[38;5;241m=\u001b[39mmpl\u001b[38;5;241m.\u001b[39mrcParams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maxes.grid.which\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m   1251\u001b[0m           axis\u001b[38;5;241m=\u001b[39mmpl\u001b[38;5;241m.\u001b[39mrcParams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maxes.grid.axis\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m   1252\u001b[0m props \u001b[38;5;241m=\u001b[39m font_manager\u001b[38;5;241m.\u001b[39mFontProperties(\n\u001b[1;32m   1253\u001b[0m     size\u001b[38;5;241m=\u001b[39mmpl\u001b[38;5;241m.\u001b[39mrcParams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maxes.titlesize\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m   1254\u001b[0m     weight\u001b[38;5;241m=\u001b[39mmpl\u001b[38;5;241m.\u001b[39mrcParams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maxes.titleweight\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/matplotlib/_api/deprecation.py:299\u001b[0m, in \u001b[0;36mrename_parameter.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    294\u001b[0m     warn_deprecated(\n\u001b[1;32m    295\u001b[0m         since, message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mold\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m parameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m() \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    296\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas been renamed \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnew\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m since Matplotlib \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msince\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m; support \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    297\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor the old name will be dropped %(removal)s.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    298\u001b[0m     kwargs[new] \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(old)\n\u001b[0;32m--> 299\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/matplotlib/axes/_base.py:3226\u001b[0m, in \u001b[0;36m_AxesBase.grid\u001b[0;34m(self, visible, which, axis, **kwargs)\u001b[0m\n\u001b[1;32m   3224\u001b[0m _api\u001b[38;5;241m.\u001b[39mcheck_in_list([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mboth\u001b[39m\u001b[38;5;124m'\u001b[39m], axis\u001b[38;5;241m=\u001b[39maxis)\n\u001b[1;32m   3225\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m axis \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mboth\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m-> 3226\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mxaxis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvisible\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhich\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwhich\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3227\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m axis \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mboth\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m   3228\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39myaxis\u001b[38;5;241m.\u001b[39mgrid(visible, which\u001b[38;5;241m=\u001b[39mwhich, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/matplotlib/_api/deprecation.py:299\u001b[0m, in \u001b[0;36mrename_parameter.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    294\u001b[0m     warn_deprecated(\n\u001b[1;32m    295\u001b[0m         since, message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mold\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m parameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m() \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    296\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas been renamed \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnew\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m since Matplotlib \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msince\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m; support \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    297\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor the old name will be dropped %(removal)s.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    298\u001b[0m     kwargs[new] \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(old)\n\u001b[0;32m--> 299\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/matplotlib/axis.py:1434\u001b[0m, in \u001b[0;36mAxis.grid\u001b[0;34m(self, visible, which, **kwargs)\u001b[0m\n\u001b[1;32m   1431\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m which \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmajor\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mboth\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m   1432\u001b[0m     gridkw[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgridOn\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m (\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_major_tick_kw[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgridOn\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m   1433\u001b[0m                         \u001b[38;5;28;01mif\u001b[39;00m visible \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m visible)\n\u001b[0;32m-> 1434\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_tick_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwhich\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmajor\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgridkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1435\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstale \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/matplotlib/axis.py:873\u001b[0m, in \u001b[0;36mAxis.set_tick_params\u001b[0;34m(self, which, reset, **kw)\u001b[0m\n\u001b[1;32m    871\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m which \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmajor\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mboth\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m    872\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_major_tick_kw\u001b[38;5;241m.\u001b[39mupdate(kwtrans)\n\u001b[0;32m--> 873\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m tick \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmajorTicks\u001b[49m:\n\u001b[1;32m    874\u001b[0m         tick\u001b[38;5;241m.\u001b[39m_apply_params(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwtrans)\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m which \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mminor\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mboth\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/matplotlib/axis.py:593\u001b[0m, in \u001b[0;36m_LazyTickList.__get__\u001b[0;34m(self, instance, cls)\u001b[0m\n\u001b[1;32m    591\u001b[0m     instance\u001b[38;5;241m.\u001b[39mmajorTicks \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    592\u001b[0m     tick \u001b[38;5;241m=\u001b[39m instance\u001b[38;5;241m.\u001b[39m_get_tick(major\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 593\u001b[0m     \u001b[43minstance\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmajorTicks\u001b[49m\u001b[38;5;241m.\u001b[39mappend(tick)\n\u001b[1;32m    594\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m instance\u001b[38;5;241m.\u001b[39mmajorTicks\n\u001b[1;32m    595\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/matplotlib/axis.py:592\u001b[0m, in \u001b[0;36m_LazyTickList.__get__\u001b[0;34m(self, instance, cls)\u001b[0m\n\u001b[1;32m    590\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_major:\n\u001b[1;32m    591\u001b[0m     instance\u001b[38;5;241m.\u001b[39mmajorTicks \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 592\u001b[0m     tick \u001b[38;5;241m=\u001b[39m \u001b[43minstance\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_tick\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmajor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    593\u001b[0m     instance\u001b[38;5;241m.\u001b[39mmajorTicks\u001b[38;5;241m.\u001b[39mappend(tick)\n\u001b[1;32m    594\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m instance\u001b[38;5;241m.\u001b[39mmajorTicks\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/matplotlib/axis.py:2057\u001b[0m, in \u001b[0;36mXAxis._get_tick\u001b[0;34m(self, major)\u001b[0m\n\u001b[1;32m   2055\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2056\u001b[0m     tick_kw \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_minor_tick_kw\n\u001b[0;32m-> 2057\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mXTick\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmajor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmajor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtick_kw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/matplotlib/axis.py:415\u001b[0m, in \u001b[0;36mXTick.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    414\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 415\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    416\u001b[0m     \u001b[38;5;66;03m# x in data coords, y in axes coords\u001b[39;00m\n\u001b[1;32m    417\u001b[0m     ax \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/matplotlib/axis.py:172\u001b[0m, in \u001b[0;36mTick.__init__\u001b[0;34m(self, axes, loc, size, width, color, tickdir, pad, labelsize, labelcolor, zorder, gridOn, tick1On, tick2On, label1On, label2On, major, labelrotation, grid_color, grid_linestyle, grid_linewidth, grid_alpha, **kw)\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgridline\u001b[38;5;241m.\u001b[39mget_path()\u001b[38;5;241m.\u001b[39m_interpolation_steps \u001b[38;5;241m=\u001b[39m \\\n\u001b[1;32m    167\u001b[0m     GRIDLINE_INTERPOLATION_STEPS\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel1 \u001b[38;5;241m=\u001b[39m mtext\u001b[38;5;241m.\u001b[39mText(\n\u001b[1;32m    169\u001b[0m     np\u001b[38;5;241m.\u001b[39mnan, np\u001b[38;5;241m.\u001b[39mnan,\n\u001b[1;32m    170\u001b[0m     fontsize\u001b[38;5;241m=\u001b[39mlabelsize, color\u001b[38;5;241m=\u001b[39mlabelcolor, visible\u001b[38;5;241m=\u001b[39mlabel1On,\n\u001b[1;32m    171\u001b[0m     rotation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_labelrotation[\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m--> 172\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel2 \u001b[38;5;241m=\u001b[39m \u001b[43mmtext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mText\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnan\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfontsize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabelsize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabelcolor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisible\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabel2On\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrotation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_labelrotation\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply_tickdir(tickdir)\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m artist \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtick1line, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtick2line, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgridline,\n\u001b[1;32m    180\u001b[0m                \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel1, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel2]:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/matplotlib/text.py:160\u001b[0m, in \u001b[0;36mText.__init__\u001b[0;34m(self, x, y, text, color, verticalalignment, horizontalalignment, multialignment, fontproperties, rotation, linespacing, rotation_mode, usetex, wrap, transform_rotates_text, parse_math, **kwargs)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_linespacing \u001b[38;5;241m=\u001b[39m linespacing\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_rotation_mode(rotation_mode)\n\u001b[0;32m--> 160\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/matplotlib/text.py:172\u001b[0m, in \u001b[0;36mText.update\u001b[0;34m(self, kwargs)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# Update bbox last, as it depends on font properties.\u001b[39;00m\n\u001b[1;32m    171\u001b[0m bbox \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbbox\u001b[39m\u001b[38;5;124m\"\u001b[39m, sentinel)\n\u001b[0;32m--> 172\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m bbox \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m sentinel:\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_bbox(bbox)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/matplotlib/artist.py:1055\u001b[0m, in \u001b[0;36mArtist.update\u001b[0;34m(self, props)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m \u001b[38;5;124;03mUpdate this artist's properties from the dict *props*.\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1052\u001b[0m \u001b[38;5;124;03mprops : dict\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1054\u001b[0m ret \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m-> 1055\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mcbook\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setattr_cm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meventson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1056\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m props\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m   1057\u001b[0m         \u001b[38;5;66;03m# Allow attributes we want to be able to update through\u001b[39;00m\n\u001b[1;32m   1058\u001b[0m         \u001b[38;5;66;03m# art.update, art.set, setp.\u001b[39;00m\n\u001b[1;32m   1059\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maxes\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/contextlib.py:281\u001b[0m, in \u001b[0;36mcontextmanager.<locals>.helper\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m    280\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhelper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[0;32m--> 281\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_GeneratorContextManager\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/contextlib.py:103\u001b[0m, in \u001b[0;36m_GeneratorContextManagerBase.__init__\u001b[0;34m(self, func, args, kwds)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, args, kwds):\n\u001b[0;32m--> 103\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgen \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwds \u001b[38;5;241m=\u001b[39m func, args, kwds\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;66;03m# Issue 19330: ensure context manager instances have good docstrings\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_start_index = 0\n",
    "plot_end_index = len(filenames3_1)\n",
    "n_plots = int(round(len(filenames3_1)/10,0))\n",
    "\n",
    "for plot_num in range(n_plots):\n",
    "    fig = plt.figure()\n",
    "    plt.ion()\n",
    "    ax = fig.add_subplot(111)\n",
    "    currentNsmax_max = []\n",
    "    index_max = []\n",
    "\n",
    "    df3 = []\n",
    "    start_j = plot_start_index + plot_num * 10\n",
    "    end_j = plot_start_index + (plot_num+1) * 10\n",
    "    \n",
    "    for j in range(start_j, end_j):\n",
    "        sample = pd.read_csv(filenames3_1[j])\n",
    "        df1 = sample[\"Incidence\"]\n",
    "        df3.append(df1)\n",
    "\n",
    "        currentNsmax = max(df1)\n",
    "        if(currentNsmax < 51):\n",
    "            lines = Line2D(list(range(len(df1))), df1, color = \"grey\")\n",
    "        else:\n",
    "            lines = Line2D(list(range(len(df1))), df1, color = \"grey\")\n",
    "        \n",
    "        ax.add_line(lines)\n",
    "        plt.ioff()\n",
    "        currentNsmax_max.append(currentNsmax)\n",
    "        \n",
    "    ax.set_xlim(0.0, 100)\n",
    "    ax.set_ylim(0.0, 100)\n",
    "    plt.ylabel(\"Daily Incidence\")\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.savefig(\"G6_at_a_time_plots_\"+str(j), dpi=300, bbox_inches='tight')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2055,
   "id": "a7e3835b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df33 = combinded3.median(axis = 1)\n",
    "df33_2_5 = combinded3.quantile(q= 0.025, axis=1)\n",
    "df33_25 = combinded3.quantile(q= 0.25, axis=1)\n",
    "df33_75 = combinded3.quantile(q= 0.75, axis=1)\n",
    "df33_97_5 = combinded3.quantile(q= 0.975, axis=1)\n",
    "\n",
    "plt.plot(range(len(df33)),list(df33), label = 'Median')\n",
    "plt.fill_between(range(len(df33)), np.array(df33)- (np.array(df33) - np.array(df33_25)), np.array(df33)+(np.array(df33_75) - np.array(df33)), alpha=0.2,edgecolor='#1B2ACC',\n",
    " facecolor='#089FFF',  label = r'$50\\%$'\" interval\")\n",
    "plt.fill_between(range(len(df33)), np.array(df33)- (np.array(df33) - np.array(df33_2_5)), np.array(df33)+(np.array(df33_97_5) - np.array(df33)), alpha =0.2, label = r'$95\\%$'\" interval\", edgecolor=(0,0,0,.4))\n",
    "plt.xlim(xmin = 0, xmax = 100)\n",
    "plt.ylim(ymin = 0, ymax = 80)\n",
    "plt.ylabel(\"Daily Incidence\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Time\")\n",
    "plt.title(r'$\\xi = 750$')\n",
    "plt.savefig(\"Daily incidence vs time seven\",dpi=300, bbox_inches='tight')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2044,
   "id": "d22fee3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "currentNsmax_max = []\n",
    "index_max = []\n",
    "\n",
    "df4 = []\n",
    "for j in range(myrange[2],myrange[3]):\n",
    "    sample = pd.read_csv(filenames3_1[j])\n",
    "    df1 = sample[\"Incidence\"]\n",
    "    df4.append(df1)\n",
    "    \n",
    "    currentNsmax = max(df1)\n",
    "    lines = Line2D(list(range(len(df1))), df1)\n",
    "    ax.add_line(lines)\n",
    "    \n",
    "    currentNsmax_max.append(currentNsmax)\n",
    "\n",
    "combinded4 = pd.concat(df4,axis=1)\n",
    "combinded4.to_csv('Incidence_vs_time_1000.csv', index = list(range(tmax)))\n",
    "\n",
    "ax.set_xlim(0.0, 200)\n",
    "ax.set_ylim(0.0, max(currentNsmax_max))\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Daily Incidence')\n",
    "plt.savefig(\"Graph_Incidence_vs_time_1000\",dpi=300, bbox_inches='tight' )\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2054,
   "id": "5ac7aa3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df44 = combinded4.median(axis = 1)\n",
    "df44_2_5 = combinded4.quantile(q= 0.025, axis=1)\n",
    "df44_25 = combinded4.quantile(q= 0.25, axis=1)\n",
    "df44_75 = combinded4.quantile(q= 0.75, axis=1)\n",
    "df44_97_5 = combinded4.quantile(q= 0.975, axis=1)\n",
    "\n",
    "\n",
    "plt.plot(range(len(df44)),list(df44), label = 'Median')\n",
    "plt.fill_between(range(len(df44)), np.array(df44)- (np.array(df44) - np.array(df44_25)), np.array(df44)+(np.array(df44_75) - np.array(df44)), alpha=0.2,edgecolor='#1B2ACC',\n",
    " facecolor='#089FFF', label =  r'$50\\%$'\" interval\")\n",
    "plt.fill_between(range(len(df44)), np.array(df44)- (np.array(df44) - np.array(df44_2_5)), np.array(df44)+(np.array(df44_97_5) - np.array(df44)), alpha =0.2, label =  r'$95\\%$'\" interval\", edgecolor=(0,0,0,.4))\n",
    "plt.ylabel(\"Daily Incidence\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Time\")\n",
    "plt.title(r'$\\xi = 1000$')\n",
    "plt.xlim(xmin = 0, xmax = 100)\n",
    "plt.ylim(ymin = 0, ymax = 80)\n",
    "plt.savefig(\"Daily incidence vs time one thou not at 365\", dpi=300, bbox_inches='tight')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2047,
   "id": "e0304214",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "currentNsmax_max = []\n",
    "\n",
    "\n",
    "df5 = []\n",
    "for j in range(myrange[3],myrange[4]):\n",
    "    sample = pd.read_csv(filenames3_1[j])\n",
    "    df1 = sample[\"Incidence\"]\n",
    "    df5.append(df1)\n",
    "    \n",
    "    currentNsmax = max(df1)\n",
    "    lines = Line2D(list(range(len(df1))), df1)\n",
    "    ax.add_line(lines)\n",
    "    \n",
    "    currentNsmax_max.append(currentNsmax)\n",
    "\n",
    "\n",
    "combinded5 = pd.concat(df5,axis=1)\n",
    "combinded5.to_csv('Incidence_vs_time_1250.csv', index = list(range(tmax)))\n",
    "\n",
    "ax.set_xlim(0.0, 200)\n",
    "ax.set_ylim(0.0, max(currentNsmax_max))\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Daily Incidence')\n",
    "plt.savefig(\"Graph_Incidence_vs_time_1250\",dpi=300, bbox_inches='tight' )\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2053,
   "id": "0d9cd0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df55 = combinded5.median(axis = 1)\n",
    "df55_mean = combinded5.mean(axis = 1)\n",
    "df55_2_5 = combinded5.quantile(q= 0.025, axis=1)\n",
    "df55_25 = combinded5.quantile(q= 0.25, axis=1)\n",
    "df55_75 = combinded5.quantile(q= 0.75, axis=1)\n",
    "df55_97_5 = combinded5.quantile(q= 0.975, axis=1)\n",
    "\n",
    "\n",
    "plt.plot(range(len(df55)),list(df55), label = 'Median')\n",
    "plt.fill_between(range(len(df55)), np.array(df55)- (np.array(df55) - np.array(df55_25)), np.array(df55)+(np.array(df55_75) - np.array(df55)), alpha=0.2,edgecolor='#1B2ACC',\n",
    " facecolor='#089FFF', label =  r'$50\\%$'\" interval\")\n",
    "plt.fill_between(range(len(df55)), np.array(df55)- (np.array(df55) - np.array(df55_2_5)), np.array(df55)+(np.array(df55_97_5) - np.array(df55)), alpha =0.2, label =  r'$95\\%$'\" interval\", edgecolor=(0,0,0,.4))\n",
    "plt.ylabel(\"Daily Incidence\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.legend()\n",
    "plt.xlim(xmin = 0, xmax = 100)\n",
    "plt.ylim(ymin = 0, ymax = 80)\n",
    "plt.title(r'$\\xi = 1250$')\n",
    "plt.savefig(\"Daily incidence vs time one two five\",dpi=300, bbox_inches='tight')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2049,
   "id": "123384a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "currentNsmax_max = []\n",
    "index_max = []\n",
    "\n",
    "df6 = []\n",
    "for j in range(myrange[4],myrange[5]):\n",
    "    sample = pd.read_csv(filenames3_1[j])\n",
    "    df1 = sample[\"Incidence\"]\n",
    "    df6.append(df1)\n",
    "    \n",
    "    currentNsmax = max(df1)\n",
    "    lines = Line2D(list(range(len(df1))), df1)\n",
    "    ax.add_line(lines)\n",
    "    \n",
    "    currentNsmax_max.append(currentNsmax)\n",
    "    index_max.append(len(df1)-1)\n",
    "\n",
    "#combinded6 = pd.concat(df6,axis=1)\n",
    "#combinded6.to_csv('Incidence_vs_time_1500.csv', index = list(range(tmax)))\n",
    "\n",
    "ax.set_xlim(0.0, 200)\n",
    "ax.set_ylim(0.0, max(currentNsmax_max))\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Daily Incidence')\n",
    "plt.savefig(\"Graph_Incidence_vs_time_1500\",dpi=300, bbox_inches='tight' )\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2052,
   "id": "4fa34e02",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df66 = combinded6.median(axis = 1)\n",
    "df66_2_5 = combinded6.quantile(q= 0.025, axis=1)\n",
    "df66_25 = combinded6.quantile(q= 0.25, axis=1)\n",
    "df66_75 = combinded6.quantile(q= 0.75, axis=1)\n",
    "df66_97_5 = combinded6.quantile(q= 0.975, axis=1)\n",
    "\n",
    "plt.plot(range(len(df66)),list(df66), label = 'Median')\n",
    "plt.fill_between(range(len(df66)), np.array(df66)- (np.array(df66) - np.array(df66_25)), np.array(df66)+(np.array(df66_75) - np.array(df66)), alpha=0.2,edgecolor='#1B2ACC',\n",
    " facecolor='#089FFF', label =  r'$50\\%$'\" interval\")\n",
    "plt.fill_between(range(len(df66)), np.array(df66)- (np.array(df66) - np.array(df66_2_5)), np.array(df66)+(np.array(df66_97_5) - np.array(df66)), alpha =0.2, label =  r'$95\\%$'\" interval\", edgecolor=(0,0,0,.4))\n",
    "plt.ylabel(\"Daily Incidence\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Time\")\n",
    "plt.title(r'$\\xi = 1500$')\n",
    "plt.ylim(ymin = 0, ymax = 80)\n",
    "plt.xlim(xmin = 0, xmax = 100)\n",
    "plt.savefig(\"Daily incidence vs time one five\",dpi=300, bbox_inches='tight')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2071,
   "id": "29f6da45",
   "metadata": {},
   "outputs": [],
   "source": [
    "df11_mean = combinded6.mean(axis = 1)\n",
    "df22_mean = combinded6.mean(axis = 1)\n",
    "df33_mean = combinded6.mean(axis = 1)\n",
    "df44_mean = combinded6.mean(axis = 1)\n",
    "df55_mean = combinded6.mean(axis = 1)\n",
    "df66_mean = combinded6.mean(axis = 1)\n",
    "\n",
    "sample = pd.read_csv(\"SIRIncidence_1.csv\")\n",
    "plt.plot(range(len(df11)), df11, color = \"blue\", label = 'Median stochastic SIR')\n",
    "plt.plot(range(len(df11_mean)), df11_mean, color = \"green\", label = 'Mean stochastic SIR')\n",
    "plt.plot(sample[\"Time1\"], sample[\"Group1\"], color = \"black\", label = 'Deterministic SIR')\n",
    "plt.ylabel(\"Daily Incidence\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Time\")\n",
    "plt.xlim(xmin=0,xmax = 100)\n",
    "plt.ylim(ymin = 0, ymax =140)\n",
    "plt.title('Control')\n",
    "plt.savefig(\"Stochastic_vs_SIR_control\",dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "plt.plot(range(len(df22)), df22, color = \"blue\", label = 'Median stochastic SIR')\n",
    "plt.plot(range(len(df22_mean)), df22_mean, color = \"green\", label = 'Mean stochastic SIR')\n",
    "plt.plot(sample[\"Time2\"], sample[\"Group2\"], color = \"black\", label = 'Deterministic SIR')\n",
    "plt.ylabel(\"Daily Incidence\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Time\")\n",
    "plt.xlim(xmin=0,xmax = 100)\n",
    "plt.ylim(ymin = 0, ymax =140)\n",
    "plt.title(r'$\\xi = 500$')\n",
    "plt.savefig(\"Stochastic_vs_SIR_five\",dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "plt.plot(range(len(df33)), df33, color = \"blue\", label = 'Median stochastic SIR')\n",
    "plt.plot(range(len(df33_mean)), df33_mean, color = \"green\", label = 'Mean stochastic SIR')\n",
    "plt.plot(sample[\"Time3\"], sample[\"Group3\"], color = \"black\", label = 'Deterministic SIR')\n",
    "plt.ylabel(\"Daily Incidence\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Time\")\n",
    "plt.xlim(xmin=0,xmax = 100)\n",
    "plt.ylim(ymin = 0, ymax =140)\n",
    "plt.title(r'$\\xi = 750$')\n",
    "plt.savefig(\"Stochastic_vs_SIR_seven_five\",dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "plt.plot(range(len(df44)), df44, color = \"blue\", label = 'Median stochastic SIR')\n",
    "plt.plot(range(len(df44_mean)), df44_mean, color = \"green\", label = 'Mean stochastic SIR')\n",
    "plt.plot(sample[\"Time4\"], sample[\"Group4\"], color = \"black\", label = 'Deterministic SIR')\n",
    "plt.ylabel(\"Daily Incidence\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Time\")\n",
    "plt.xlim(xmin=0,xmax = 100)\n",
    "plt.ylim(ymin = 0, ymax =140)\n",
    "plt.title(r'$\\xi = 1000$')\n",
    "plt.savefig(\"Stochastic_vs_SIR_one_thou\",dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "plt.plot(range(len(df55)), df55, color = \"blue\", label = 'Median stochastic SIR')\n",
    "plt.plot(range(len(df55_mean)), df55_mean, color = \"green\", label = 'Mean stochastic SIR')\n",
    "plt.plot(sample[\"Time5\"], sample[\"Group5\"], color = \"black\", label = 'Deterministic SIR')\n",
    "plt.ylabel(\"Daily Incidence\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Time\")\n",
    "plt.xlim(xmin=0,xmax = 100)\n",
    "plt.ylim(ymin = 0, ymax =140)\n",
    "plt.title(r'$\\xi = 1250$')\n",
    "plt.savefig(\"Stochastic_vs_SIR_one_thou_two\",dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "plt.plot(range(len(df66)), df66, color = \"blue\", label = 'Median stochastic SIR')\n",
    "plt.plot(range(len(df66_mean)), df66_mean, color = \"green\", label = 'Mean stochastic SIR')\n",
    "plt.plot(sample[\"Time6\"], sample[\"Group6\"], color = \"black\", label = 'Deterministic SIR')\n",
    "plt.ylabel(\"Daily Incidence\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Time\")\n",
    "plt.xlim(xmin=0,xmax = 100)\n",
    "plt.ylim(ymin = 0, ymax =140)\n",
    "plt.title(r'$\\xi = 1500$')\n",
    "plt.savefig(\"Stochastic_vs_SIR_one_thou_five\",dpi=300, bbox_inches='tight')\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1885,
   "id": "e2430001",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(sample[\"Time1\"], sample[\"Group1\"], color = \"black\", label = 'Group 1')\n",
    "plt.plot(sample[\"Time2\"], sample[\"Group2\"], color = \"purple\", label = 'Group 2')\n",
    "plt.plot(sample[\"Time3\"], sample[\"Group3\"], color = \"red\", label = 'Group 3')\n",
    "plt.plot(sample[\"Time4\"], sample[\"Group4\"], color = \"green\", label = 'Group 4')\n",
    "plt.plot(sample[\"Time5\"], sample[\"Group5\"], color = \"blue\", label = 'Group 5')\n",
    "plt.plot(sample[\"Time6\"], sample[\"Group6\"], color = \"grey\", label = 'Group 6')\n",
    "plt.ylabel(\"Daily Incidence\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Time\")\n",
    "plt.xlim(xmin=0,xmax = 100)\n",
    "plt.savefig(\"SIR_vs_Groups\",dpi=300, bbox_inches='tight')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1911,
   "id": "07c6f390",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(43.0, 43.0, 43.0, 42.0, 42.0, 42.0)"
      ]
     },
     "execution_count": 1911,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(df11),max(df22),max(df33),max(df44),max(df55),max(df66) #median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1913,
   "id": "49b07ca7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(34.26589595375722,\n",
       " 34.26589595375722,\n",
       " 34.26589595375722,\n",
       " 34.26589595375722,\n",
       " 34.26589595375722,\n",
       " 34.26589595375722)"
      ]
     },
     "execution_count": 1913,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(df11_mean),max(df22_mean),max(df33_mean),max(df44_mean),max(df55_mean),max(df66_mean) #average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1914,
   "id": "4fb138e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100.934375434724,\n",
       " 101.129983385135,\n",
       " 102.62334322428,\n",
       " 114.89339776964,\n",
       " 113.423703738994,\n",
       " 121.606340496726)"
      ]
     },
     "execution_count": 1914,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(sample[\"Group1\"]),max(sample[\"Group2\"]),max(sample[\"Group3\"]),max(sample[\"Group4\"]),max(sample[\"Group5\"]),max(sample[\"Group6\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2104,
   "id": "b3acd42e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15.4]\n",
      "[15.38]\n",
      "[15.16]\n",
      "[13.68]\n",
      "[13.84]\n",
      "[12.98]\n"
     ]
    }
   ],
   "source": [
    "print(sample[sample[\"Group1\"] == max(sample[\"Group1\"])].Time1.values)\n",
    "print(sample[sample[\"Group2\"] == max(sample[\"Group2\"])].Time2.values)\n",
    "print(sample[sample[\"Group3\"] == max(sample[\"Group3\"])].Time3.values)\n",
    "print(sample[sample[\"Group4\"] == max(sample[\"Group4\"])].Time4.values)\n",
    "print(sample[sample[\"Group5\"] == max(sample[\"Group5\"])].Time5.values)\n",
    "print(sample[sample[\"Group6\"] == max(sample[\"Group6\"])].Time6.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2101,
   "id": "31ed3e39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(365.0, 365.0, 365.0, 365.0, 365.0, 365.0)"
      ]
     },
     "execution_count": 2101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample[\"Group1\"]),max(sample[\"Group2\"]),max(sample[\"Group3\"]),max(sample[\"Group4\"]),max(sample[\"Group5\"]),max(sample[\"Group6\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef2e3b2",
   "metadata": {},
   "source": [
    "Histograph of peak incidence and time of peak incidence for epidemics that took hold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2058,
   "id": "c6520733",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(peak_incidence, color = \"black\", bins = 30)\n",
    "#plt.title(\"Peak incidence per simulation\")\n",
    "plt.xlabel(\"Peak incidence\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.savefig(\"Histogram_peak_incidence_per_simulation\",dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "plt.hist(time_peak_incidence, color = \"black\", bins = 30)\n",
    "#plt.title(\"Time of peak incidence per simulation\")\n",
    "plt.xlabel(\"Time of peak incidence\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.savefig(\"Histogram_time_of_peak_incidence_per_simulation\", dpi=300, bbox_inches='tight')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1931,
   "id": "50f6f077",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(65, 5)"
      ]
     },
     "execution_count": 1931,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(time_peak_incidence), min(time_peak_incidence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64ee46e",
   "metadata": {},
   "source": [
    "Store peak incidence and time of peak incidence data per network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1956,
   "id": "7bcc769e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import accumulate\n",
    "y = [len(x) for x in Epidemics_more_than_10] \n",
    "peak_incidence_pn = [peak_incidence[x - y: x] for x, y in zip(\n",
    "        accumulate(y), y)]\n",
    "time_peak_incidence_pn = [time_peak_incidence[x - y: x] for x, y in zip(\n",
    "        accumulate(y), y)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1957,
   "id": "15e9fd50",
   "metadata": {},
   "outputs": [],
   "source": [
    "peak_incidence_rolling_mean_3_pn = [peak_incidence_rolling_mean_3[x - y: x] for x, y in zip(\n",
    "        accumulate(y), y)]\n",
    "time_peak_incidence_rolling_mean_3_pn = [time_peak_incidence_rolling_mean_3[x - y: x] for x, y in zip(\n",
    "        accumulate(y), y)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1756,
   "id": "a7adddd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(net):\n",
    "    p_i_pn = peak_incidence_pn[i]\n",
    "    t_p_i = time_peak_incidence_pn[i]\n",
    "    \n",
    "    dt = {'Peak Incidence': p_i_pn, 'Time at peak incidence': t_p_i }\n",
    "    dttt = pd.DataFrame(dt)\n",
    "    dttt.to_csv('Incidence_parameters_pn_'+str(i)+'.csv', index = list(range(no_sim)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0a19d0",
   "metadata": {},
   "source": [
    "Average incidence per network, average time for incidence per network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2079,
   "id": "c1fcbb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Av_peak_incidence_per_network = []\n",
    "Av_time_peak_incidence_per_network=[]\n",
    "median_peak_incidence_per_network = []\n",
    "median_time_peak_incidence_per_network=[]\n",
    "\n",
    "for i in range(net):\n",
    "    a_p_i_pn = np.sum(peak_incidence_pn[i])/len(peak_incidence_pn[i])\n",
    "    a_t_p_i = np.sum(time_peak_incidence_pn[i])/len(time_peak_incidence_pn[i])\n",
    "    m_p_i_pn = np.median(peak_incidence_pn[i])\n",
    "    m_t_p_i = np.median(time_peak_incidence_pn[i])\n",
    "    \n",
    "    Av_peak_incidence_per_network.append(a_p_i_pn)\n",
    "    Av_time_peak_incidence_per_network.append(a_t_p_i)\n",
    "    \n",
    "    median_peak_incidence_per_network.append(m_p_i_pn)\n",
    "    median_time_peak_incidence_per_network.append(m_t_p_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1928,
   "id": "51c9d6e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54.111111111111114, 45.49342105263158)"
      ]
     },
     "execution_count": 1928,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(Av_peak_incidence_per_network), min(Av_peak_incidence_per_network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1965,
   "id": "faf26d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "median_peak_incidence_rolling_mean_3_per_network = []\n",
    "av_peak_incidence_rolling_mean_3_per_network = []\n",
    "\n",
    "Av_time_peak_incidence_rolling_mean_3_per_network=[]\n",
    "median_time_peak_incidence_rolling_mean_3_per_network=[]\n",
    "\n",
    "for i in range(net):\n",
    "    a_p_i_pn = np.mean(peak_incidence_rolling_mean_3_pn[i])\n",
    "    a_t_p_i = np.mean(time_peak_incidence_rolling_mean_3_pn[i])\n",
    "    m_p_i_pn = np.median(peak_incidence_rolling_mean_3_pn[i])\n",
    "    m_t_p_i = np.median(time_peak_incidence_rolling_mean_3_pn[i])\n",
    "    \n",
    "    av_peak_incidence_rolling_mean_3_per_network.append(a_p_i_pn)\n",
    "    Av_time_peak_incidence_rolling_mean_3_per_network.append(a_t_p_i)\n",
    "    \n",
    "    median_peak_incidence_rolling_mean_3_per_network.append(m_p_i_pn)\n",
    "    median_time_peak_incidence_rolling_mean_3_per_network.append(m_t_p_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e649bc2",
   "metadata": {},
   "source": [
    "Incidence results for networks with the same $\\xi$ value and control networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1758,
   "id": "7d4e221a",
   "metadata": {},
   "outputs": [],
   "source": [
    "peak_incidence_pg = list(np.array_split(np.array(peak_incidence), group)) \n",
    "time_peak_incidence_pg = list(np.array_split(np.array(time_peak_incidence), group))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1917,
   "id": "cd3888c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "av_peak_incidence = [np.sum(i)/len(i) for i in peak_incidence_pg]\n",
    "av_time_peak_incidence = [np.sum(i)/len(i) for i in time_peak_incidence_pg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1988,
   "id": "bcfd3c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "median_peak_incidence = [np.median(i) for i in peak_incidence_pg]\n",
    "median_time_peak_incidence = [np.median(i) for i in time_peak_incidence_pg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1938,
   "id": "94a48eaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[64.75, 62.25, 62.25, 61.875, 59.25, 57.0]\n",
      "[37.625, 35.75, 38.625, 34.375, 1.0, 40.0]\n"
     ]
    }
   ],
   "source": [
    "max_peak_inc = [max(i) for i in peak_incidence_pg]\n",
    "min_peak_inc = [min(i) for i in peak_incidence_pg]\n",
    "print(max_peak_inc)\n",
    "print(min_peak_inc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1934,
   "id": "44334618",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[48, 51, 55, 65, 52, 56]"
      ]
     },
     "execution_count": 1934,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_time_peak_incidence =[max(i) for i in time_peak_incidence_pg]\n",
    "max_time_peak_incidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1936,
   "id": "81dc9846",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7, 6, 7, 6, 5, 6]"
      ]
     },
     "execution_count": 1936,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_time_peak_incidence = [min(i) for i in time_peak_incidence_pg]\n",
    "min_time_peak_incidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1930,
   "id": "02f96137",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([18., 18., 18., 16., 15., 14.])"
      ]
     },
     "execution_count": 1930,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(av_time_peak_incidence,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1922,
   "id": "b411de9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([17.76, 17.9 , 17.5 , 16.31, 15.46, 14.26])"
      ]
     },
     "execution_count": 1922,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(av_time_peak_incidence,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1990,
   "id": "21f7b49b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([17. , 16. , 16. , 14.5, 13. , 12. ])"
      ]
     },
     "execution_count": 1990,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(median_time_peak_incidence,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1989,
   "id": "66e7b846",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([52.38, 51.62, 51.25, 50.94, 50.25, 48.81])"
      ]
     },
     "execution_count": 1989,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(median_peak_incidence,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5281f2b9",
   "metadata": {},
   "source": [
    "Store results for network \"groups\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1759,
   "id": "e82b8633",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(group):\n",
    "    p_i_pg = peak_incidence_pn[i]\n",
    "    t_p_i_pg = time_peak_incidence_pn[i]\n",
    "    data22 = {'Peak Incidence': p_i_pg, 'Time at peak incidence': t_p_i_pg }\n",
    "    df22 = pd.DataFrame(data22)\n",
    "    df22.to_csv('Incidence_parameters_pg_'+str(i)+'.csv', index = list(range(sim_p_group)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb6343b",
   "metadata": {},
   "source": [
    "Calculate variance, percentiles, and std for average peak incidence per network and store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1760,
   "id": "fd6e0a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "peak_incidence_variance = []\n",
    "incidence_percentile_95 = []\n",
    "incidence_percentile_5 =[]\n",
    "incidence_std = []\n",
    "for i in range(net):\n",
    "    var = np.var(peak_incidence_pn[i])\n",
    "    peak_incidence_variance.append(var)\n",
    "    per95 = np.percentile(peak_incidence_pn[i],95)\n",
    "    incidence_percentile_95.append(per95)\n",
    "    per5 = np.percentile(peak_incidence_pn[i],5)\n",
    "    incidence_percentile_5.append(per5)\n",
    "    sd = np.std(peak_incidence_pn[i], ddof=1)/np.sqrt(len(peak_incidence_pn[i]))\n",
    "    incidence_std.append(sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1991,
   "id": "0e51834f",
   "metadata": {},
   "outputs": [],
   "source": [
    "peak_incidence_variance_pg = list(np.array_split(np.array(peak_incidence_variance), group)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1993,
   "id": "bd7942d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "av_var_peak_incidence_pg = [np.mean(i) for i in peak_incidence_variance_pg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1960,
   "id": "cd43d8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "peak_incidence_rolling_mean_3_variance = []\n",
    "incidence_rolling_mean_3_percentile_95 = []\n",
    "incidence_rolling_mean_3_percentile_5 =[]\n",
    "incidence_rolling_mean_3_std = []\n",
    "for i in range(net):\n",
    "    var = np.var(peak_incidence_rolling_mean_3_pn[i])\n",
    "    peak_incidence_rolling_mean_3_variance.append(var)\n",
    "    per95 = np.percentile(peak_incidence_rolling_mean_3_pn[i],95)\n",
    "    incidence_rolling_mean_3_percentile_95.append(per95)\n",
    "    per5 = np.percentile(peak_incidence_rolling_mean_3_pn[i],5)\n",
    "    incidence_rolling_mean_3_percentile_5.append(per5)\n",
    "    sd = np.std(peak_incidence_rolling_mean_3_pn[i], ddof=1)/np.sqrt(len(peak_incidence_rolling_mean_3_pn[i]))\n",
    "    incidence_rolling_mean_3_std.append(sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1761,
   "id": "e9c77ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = list(range(1,net+1))\n",
    "data9 = {'Average peak incidence per network': Av_peak_incidence_per_network,\n",
    "        'Variance':  peak_incidence_variance,\n",
    "        '5th percentile': incidence_percentile_5,\n",
    "        '95th percentile': incidence_percentile_95,\n",
    "        'Standard deviation': incidence_std}\n",
    "df9 = pd.DataFrame(data9)\n",
    "df9.to_csv('Average_peak_incidence_pn_vs_variance.csv', index = ind)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d2a5db",
   "metadata": {},
   "source": [
    "Peak incidence per network vs motifs, with percentiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1762,
   "id": "da2175f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "yerr11=[np.array(Av_peak_incidence_per_network)- np.array(incidence_percentile_5), np.array(incidence_percentile_95)-np.array(Av_peak_incidence_per_network)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1975,
   "id": "f250ce06",
   "metadata": {},
   "outputs": [],
   "source": [
    "yerr11_33=[np.array(median_peak_incidence_per_network)- np.array(incidence_percentile_5), np.array(incidence_percentile_95)-np.array(median_peak_incidence_per_network)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2133,
   "id": "c3b47b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.errorbar(Triangle_motif,  Av_peak_incidence_per_network, yerr=yerr11, markersize = 4, fmt = 'o',capsize=2, color='black',\n",
    "             ecolor='gray', elinewidth=1)\n",
    "plt.xlabel(\"Number of triangle motifs\")\n",
    "plt.ylabel(\"Average peak incidence\")\n",
    "plt.ylim(ymin = 0)\n",
    "#plt.title(\"The average peak incidence vs the number of Triangle motifs per network\")\n",
    "plt.savefig(\"Average_peak_incidence_vs_triangles\", dpi=300, bbox_inches='tight')\n",
    "#plt.yscale(\"log\")\n",
    "plt.close()\n",
    "\n",
    "plt.errorbar(Diamond_motif,  Av_peak_incidence_per_network, yerr=yerr11, markersize = 4, fmt = 'o',capsize=2, color='black',\n",
    "             ecolor='gray', elinewidth=1)\n",
    "plt.xlabel(\"Number of diamond motifs\")\n",
    "plt.ylabel(\"Average peak incidence\")\n",
    "#plt.title(\"The average peak incidence vs the number of Diamond motifs per network\")\n",
    "plt.ylim(ymin = 0)\n",
    "plt.savefig(\"Average_peak_incidence_vs_Diamonds\",dpi=300, bbox_inches='tight')\n",
    "#plt.yscale(\"log\")\n",
    "plt.close()\n",
    "\n",
    "plt.errorbar(Tadpole_motif,  Av_peak_incidence_per_network, yerr=yerr11, markersize = 4, fmt = 'o',capsize=2, color='black',\n",
    "             ecolor='gray', elinewidth=1)\n",
    "plt.xlabel(\"Number of tadpole motifs\")\n",
    "plt.ylabel(\"Average peak incidence\")\n",
    "plt.ylim(ymin = 0)\n",
    "#plt.title(\"The average peak incidence vs the number of Tadpole motifs per network\")\n",
    "plt.savefig(\"Average_peak_incidence_vs_Tadpoles\",dpi=300, bbox_inches='tight')\n",
    "#plt.yscale(\"log\")\n",
    "plt.close()\n",
    "\n",
    "\n",
    "plt.errorbar(Bowtie_motif,  Av_peak_incidence_per_network, yerr=yerr11, markersize = 4, fmt = 'o',capsize=2, color='black',\n",
    "             ecolor='gray', elinewidth=1)\n",
    "plt.xlabel(\"Number of bowtie motifs\")\n",
    "plt.ylabel(\"Average peak incidence\")\n",
    "plt.ylim(ymin = 0)\n",
    "#plt.title(\"The average peak incidence vs the number of Bowtie motifs per network\")\n",
    "plt.savefig(\"Average_peak_incidence_vs_Bowtie\",dpi=300, bbox_inches='tight')\n",
    "#plt.yscale(\"log\")\n",
    "plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2059,
   "id": "8079d864",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.errorbar(Triangle_motif, median_peak_incidence_per_network, yerr=yerr11_33, markersize = 4, fmt = 'o',capsize=2, color='black',\n",
    "             ecolor='gray', elinewidth=1)\n",
    "plt.xlabel(\"Number of Triangle motifs\")\n",
    "plt.ylabel(\"Peak incidence\")\n",
    "plt.ylim(ymin = 0,ymax=70)\n",
    "plt.xlim(xmin=0)\n",
    "#plt.title(\"The average peak incidence vs the number of Triangle motifs per network\")\n",
    "plt.savefig(\"Median_peak_incidence_vs_triangles\", dpi=300, bbox_inches='tight')\n",
    "#plt.yscale(\"log\")\n",
    "plt.close()\n",
    "\n",
    "plt.errorbar(Diamond_motif,  median_peak_incidence_per_network, yerr=yerr11_33, markersize = 4, fmt = 'o',capsize=2, color='black',\n",
    "             ecolor='gray', elinewidth=1)\n",
    "plt.xlabel(\"Number of Diamond motifs\")\n",
    "plt.ylabel(\"Peak incidence\")\n",
    "#plt.title(\"The average peak incidence vs the number of Diamond motifs per network\")\n",
    "plt.ylim(ymin = 0,ymax=70)\n",
    "plt.xlim(xmin=0)\n",
    "plt.savefig(\"Median_peak_incidence_vs_Diamonds\",dpi=300, bbox_inches='tight')\n",
    "#plt.yscale(\"log\")\n",
    "plt.close()\n",
    "\n",
    "plt.errorbar(Tadpole_motif,  median_peak_incidence_per_network, yerr=yerr11_33, markersize = 4, fmt = 'o',capsize=2, color='black',\n",
    "             ecolor='gray', elinewidth=1)\n",
    "plt.xlabel(\"Number of Tadpole motifs\")\n",
    "plt.ylabel(\"Peak incidence\")\n",
    "plt.ylim(ymin = 0,ymax=70)\n",
    "plt.xlim(xmin=0)\n",
    "#plt.title(\"The average peak incidence vs the number of Tadpole motifs per network\")\n",
    "plt.savefig(\"Median_peak_incidence_vs_Tadpoles\",dpi=300, bbox_inches='tight')\n",
    "#plt.yscale(\"log\")\n",
    "plt.close()\n",
    "\n",
    "\n",
    "plt.errorbar(Bowtie_motif,  median_peak_incidence_per_network, yerr=yerr11_33, markersize = 4, fmt = 'o',capsize=2, color='black',\n",
    "             ecolor='gray', elinewidth=1)\n",
    "plt.xlabel(\"Number of Bowtie motifs\")\n",
    "plt.ylabel(\"Peak incidence\")\n",
    "plt.ylim(ymin = 0,ymax=70)\n",
    "plt.xlim(xmin=0)\n",
    "#plt.title(\"The average peak incidence vs the number of Bowtie motifs per network\")\n",
    "plt.savefig(\"Median_peak_incidence_vs_Bowtie\",dpi=300, bbox_inches='tight')\n",
    "#plt.yscale(\"log\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd619f24",
   "metadata": {},
   "source": [
    "Variance, std and percentiles for time of peak incidence per network and store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1764,
   "id": "130edde9",
   "metadata": {},
   "outputs": [],
   "source": [
    "peak_incidence_time_variance = []\n",
    "incidence_time_percentile_95 = []\n",
    "incidence_time_percentile_5 =[]\n",
    "incidence_time_std = []\n",
    "for i in range(net):\n",
    "    var = np.var(time_peak_incidence_pn[i])\n",
    "    peak_incidence_time_variance.append(var)\n",
    "    per95 = np.percentile(time_peak_incidence_pn[i],95)\n",
    "    incidence_time_percentile_95.append(per95)\n",
    "    per5 = np.percentile(time_peak_incidence_pn[i],5)\n",
    "    incidence_time_percentile_5.append(per5)\n",
    "    sd = np.std(time_peak_incidence_pn[i], ddof=1)/np.sqrt(len(time_peak_incidence_pn[i]))\n",
    "    incidence_time_std.append(sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1998,
   "id": "fbb29c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "peak_incidence_time_variance_pg = list(np.array_split(np.array(peak_incidence_time_variance), group)) \n",
    "av_peak_incidence_time_variance_pg = [np.mean(i) for i in peak_incidence_time_variance_pg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1999,
   "id": "2334b7f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[32.209655828361285, 44.06842845683125, 43.67122012721817, 53.17891824046581, 46.56541255037594, 49.809052853003735]\n",
      "[20.582758471301386, 17.206250229133705, 17.27769388767252, 19.070316044642393, 10.52968343907131, 8.269466215871162]\n"
     ]
    }
   ],
   "source": [
    "print(av_peak_incidence_time_variance_pg)\n",
    "print(av_var_peak_incidence_pg )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1765,
   "id": "35c3f2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = [min(x) for x in time_peak_incidence_pn]\n",
    "p = [max(x) for x in time_peak_incidence_pn]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1766,
   "id": "0a6b1bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.round(incidence_time_percentile_95,0)\n",
    "#np.round(incidence_time_percentile_5,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1767,
   "id": "26a13ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = list(range(1,net+1))\n",
    "data91 = {'Average time at peak incidence per network': Av_time_peak_incidence_per_network,\n",
    "        'Variance':  peak_incidence_time_variance,\n",
    "        '5th percentile': incidence_time_percentile_5,\n",
    "        '95th percentile': incidence_time_percentile_95,\n",
    "        'Standard deviation': incidence_time_std}\n",
    "df91 = pd.DataFrame(data91)\n",
    "df91.to_csv('Average_peak_time_incidence_pn_vs_variance.csv', index = ind)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813f4c71",
   "metadata": {},
   "source": [
    "Average time of peak incidence per network, with percentiles plotted against motifs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2084,
   "id": "33160f36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16.85, 16.583333333333332, 15.75, 14.233333333333333, 13.2, 11.866666666666667]\n",
      "[52.4375, 51.63125, 51.545833333333334, 50.979166666666664, 50.05416666666667, 48.59583333333333]\n"
     ]
    }
   ],
   "source": [
    "median_time_peak_incidence_per_network_pg = list(np.array_split(np.array(median_time_peak_incidence_per_network), group))\n",
    "median_peak_incidence_per_network_pg = list(np.array_split(np.array(median_peak_incidence_per_network), group))\n",
    "\n",
    "av_median_time_peak_incidence_per_network_pg = [np.mean(i) for i in median_time_peak_incidence_per_network_pg]\n",
    "av_median_peak_incidence_per_network_pg = [np.mean(i) for i in median_peak_incidence_per_network_pg]\n",
    "\n",
    "print(av_median_time_peak_incidence_per_network_pg)\n",
    "print(av_median_peak_incidence_per_network_pg )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66afa92b",
   "metadata": {},
   "source": [
    "plot time of peak incidence per network vs triangles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1977,
   "id": "94558c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "yerr_t11=[np.array(median_time_peak_incidence_per_network)- np.array(incidence_time_percentile_5), np.array(incidence_time_percentile_95)-np.array(median_time_peak_incidence_per_network)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2134,
   "id": "cf110f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.errorbar(Triangle_motif, Av_time_peak_incidence_per_network, yerr= yerr_t11, markersize = 4, fmt = 'o',capsize=3,color='black',\n",
    "             ecolor='gray', elinewidth=1)\n",
    "plt.xlabel(\"Number of triangle motifs\")\n",
    "plt.ylabel(\"Average time of peak incidence\")\n",
    "#plt.title(\"Average time of peak incidence vs the number of Triangle motifs per network\")\n",
    "plt.ylim(ymin=0)\n",
    "plt.savefig(\"Average_time_of_peak_incidence_vs_triangles\",dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "plt.errorbar(Diamond_motif,Av_time_peak_incidence_per_network, yerr=yerr_t11, markersize = 4, fmt = 'o',capsize=3,color='black',\n",
    "             ecolor='gray', elinewidth=1)\n",
    "plt.xlabel(\"Number of diamond motifs\")\n",
    "plt.ylabel(\"Average time of peak incidence\")\n",
    "plt.ylim(ymin=0)\n",
    "#plt.title(\"Average time of peak incidence vs the number of Diamond motifs per network\")\n",
    "plt.savefig(\"Average_time_of_peak_incidence_vs_Diamonds\",dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "plt.errorbar(Tadpole_motif,Av_time_peak_incidence_per_network, yerr=yerr_t11, markersize = 4, fmt = 'o',capsize=3,color='black',\n",
    "             ecolor='gray', elinewidth=1)\n",
    "plt.xlabel(\"Number of tadpole motifs\")\n",
    "plt.ylabel(\"Average time of peak incidence\")\n",
    "plt.ylim(ymin=0)\n",
    "#plt.title(\"Average time of peak incidence vs the number of Tadpole motifs per network\")\n",
    "plt.savefig(\"Average_time_of_peak_incidence_vs_Tadpoles\",dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "plt.errorbar(Bowtie_motif,Av_time_peak_incidence_per_network, yerr=yerr_t11, markersize = 4, fmt = 'o',capsize=3,color='black',\n",
    "             ecolor='gray', elinewidth=1)\n",
    "plt.xlabel(\"Number of bowtie motifs\")\n",
    "plt.ylabel(\"Average time of peak incidence\")\n",
    "plt.ylim(ymin=0)\n",
    "#plt.title(\"Average time of peak incidence vs the number of Bowtie motifs per network\")\n",
    "plt.savefig(\"Average_time_of_peak_incidence_vs_Bowties\",dpi=300, bbox_inches='tight')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2135,
   "id": "7c86b578",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.errorbar(Triangle_motif, median_time_peak_incidence_per_network, yerr= yerr_t11, markersize = 4, fmt = 'o',capsize=3,color='black',\n",
    "             ecolor='gray', elinewidth=1)\n",
    "plt.xlabel(\"Number of triangle motifs\")\n",
    "plt.ylabel(\"Time of peak incidence\")\n",
    "#plt.title(\"Average time of peak incidence vs the number of Triangle motifs per network\")\n",
    "plt.ylim(ymin=0)\n",
    "plt.xlim(xmin=0)\n",
    "plt.savefig(\"median_time_of_peak_incidence_vs_triangles\",dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "plt.errorbar(Diamond_motif,median_time_peak_incidence_per_network, yerr=yerr_t11, markersize = 4, fmt = 'o',capsize=3,color='black',\n",
    "             ecolor='gray', elinewidth=1)\n",
    "plt.xlabel(\"Number of diamond motifs\")\n",
    "plt.ylabel(\"Time of peak incidence\")\n",
    "plt.ylim(ymin=0)\n",
    "plt.xlim(xmin=0)\n",
    "#plt.title(\"Average time of peak incidence vs the number of Diamond motifs per network\")\n",
    "plt.savefig(\"median_time_of_peak_incidence_vs_Diamonds\",dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "plt.errorbar(Tadpole_motif,median_time_peak_incidence_per_network, yerr=yerr_t11, markersize = 4, fmt = 'o',capsize=3,color='black',\n",
    "             ecolor='gray', elinewidth=1)\n",
    "plt.xlabel(\"Number of tadpole motifs\")\n",
    "plt.ylabel(\"Time of peak incidence\")\n",
    "plt.ylim(ymin=0)\n",
    "plt.xlim(xmin=0)\n",
    "#plt.title(\"Average time of peak incidence vs the number of Tadpole motifs per network\")\n",
    "plt.savefig(\"median_time_of_peak_incidence_vs_Tadpoles\",dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "plt.errorbar(Bowtie_motif,median_time_peak_incidence_per_network, yerr=yerr_t11, markersize = 4, fmt = 'o',capsize=3,color='black',\n",
    "             ecolor='gray', elinewidth=1)\n",
    "plt.xlabel(\"Number of bowtie motifs\")\n",
    "plt.ylabel(\"Time of peak incidence\")\n",
    "plt.ylim(ymin=0)\n",
    "plt.xlim(xmin=0)\n",
    "#plt.title(\"Average time of peak incidence vs the number of Bowtie motifs per network\")\n",
    "plt.savefig(\"median_time_of_peak_incidence_vs_Bowties\",dpi=300, bbox_inches='tight')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03c4da3",
   "metadata": {},
   "source": [
    "### Time to 90% of total cases of infection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b87c3ed",
   "metadata": {},
   "source": [
    "Note: index_no_ep is the simulation number for an epidemic that did not take hold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2142,
   "id": "03c78fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_to_90 = []\n",
    "for j in range(no_tot_sim):\n",
    "    \n",
    "    if j not in index_no_ep: \n",
    "        sample = pd.read_csv(filenames2[j])\n",
    "        infected = list(sample[\"Infected\"])\n",
    "        recovered = list(sample[\"Recovered\"])\n",
    "        susceptible = list(sample[\"Susceptible\"])\n",
    "        time = list(sample[\"Time\"])\n",
    "    \n",
    "    \n",
    "        percent_90_infected = round(recovered[-1]*0.9)\n",
    "        y = susceptible[0] - percent_90_infected\n",
    "        index = susceptible.index(y)\n",
    "        time_to_90.append(time[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c76b304",
   "metadata": {},
   "source": [
    "Calculating the number of epidemics that took hold for each network, making a list of times to $90\\%$ of cases of infection for each network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2143,
   "id": "07a96941",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import accumulate\n",
    "y = [len(x) for x in Epidemics_more_than_10] \n",
    "time_to_90_pn = [time_to_90[x - y: x] for x, y in zip(\n",
    "        accumulate(y), y)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2144,
   "id": "74d447ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_to_90_variance = []\n",
    "time_to_90_percentile_95 = []\n",
    "time_to_90_percentile_5 =[]\n",
    "time_to_90_std = []\n",
    "\n",
    "for i in range(net):\n",
    "    var = np.var(time_to_90_pn[i])\n",
    "    time_to_90_variance.append(var)\n",
    "    \n",
    "    per95 = np.percentile(time_to_90_pn[i],95)\n",
    "    time_to_90_percentile_95.append(per95)\n",
    "    \n",
    "    per5 = np.percentile(time_to_90_pn[i],5)\n",
    "    time_to_90_percentile_5.append(per5)\n",
    "    \n",
    "    sd = np.std(time_to_90_pn[i], ddof=1)/np.sqrt(len(time_to_90_pn[i]))\n",
    "    time_to_90_std.append(sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2145,
   "id": "d58a7446",
   "metadata": {},
   "outputs": [],
   "source": [
    "av_time_to_90 = []\n",
    "for i in time_to_90_pn:\n",
    "    p = np.sum(i)/len(i)\n",
    "    av_time_to_90.append(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2147,
   "id": "5c0c44a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "median_time_to_90 = []\n",
    "for i in time_to_90_pn:\n",
    "    p = np.median(i)\n",
    "    median_time_to_90.append(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2150,
   "id": "a7c5f815",
   "metadata": {},
   "outputs": [],
   "source": [
    "median_time_to_90_pg = list(np.array_split(np.array(mean_time_to_90), group)) \n",
    "var_av_time_to_90 = [np.var(i) for i in median_time_to_90_pg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2151,
   "id": "58ccb0ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.9758662302596135,\n",
       " 3.2264921640502964,\n",
       " 4.79973220915955,\n",
       " 4.917301965228109,\n",
       " 4.055655669390244,\n",
       " 4.399758725695431]"
      ]
     },
     "execution_count": 2151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var_av_time_to_90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1945,
   "id": "60442e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "av_time_to_90_pg = list(np.array_split(np.array(av_time_to_90), group)) \n",
    "max_av_time_to_90 = [max(i) for i in av_time_to_90_pg]\n",
    "min_av_time_to_90 = [min(i) for i in av_time_to_90_pg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2099,
   "id": "91b6c0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_to_90_variance_pg = list(np.array_split(np.array(time_to_90_variance), group)) \n",
    "av_time_to_90_variance_pg = [np.mean(i) for i in time_to_90_variance_pg ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2100,
   "id": "54bcb16b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[34.11129098551358,\n",
       " 45.45890036379312,\n",
       " 44.75694402417943,\n",
       " 54.93981974861988,\n",
       " 47.959277663977296,\n",
       " 52.51414205184788]"
      ]
     },
     "execution_count": 2100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "av_time_to_90_variance_pg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1782,
   "id": "3822b1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = list(range(1,net+1))\n",
    "data91 = {'Average time to 90 cases of total infection pn':av_time_to_90,\n",
    "        'Variance':  time_to_90_variance,\n",
    "        '5th percentile': time_to_90_percentile_5 ,\n",
    "        '95th percentile': time_to_90_percentile_95,\n",
    "        'Standard deviation': time_to_90_std}\n",
    "df91 = pd.DataFrame(data91)\n",
    "df91.to_csv('Average_time_to_90_cases_infection_pn_vs_variance.csv', index = ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1939,
   "id": "6ac28128",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(time_to_90, align = \"left\", color = \"black\", bins = 28)\n",
    "plt.xlabel(\"Average time to 90$\\%$ of total cases of infection\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.savefig(\"histogram_av_time_to_90\",dpi=300, bbox_inches='tight')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2148,
   "id": "47d298f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "yerr_tn1 =[np.array(median_time_to_90)- np.array(time_to_90_percentile_5), np.array(time_to_90_percentile_95)-np.array(median_time_to_90)]\n",
    "\n",
    "plt.errorbar(Triangle_motif, median_time_to_90, yerr= yerr_tn1, markersize = 4, fmt = 'o',capsize=3,color='black',\n",
    "             ecolor='gray', elinewidth=1)\n",
    "plt.xlabel(\"Number of triangle motifs\")\n",
    "plt.ylabel('Time to ' + r'$90\\%$' + ' of total infections')\n",
    "#plt.title(\"Average time of peak incidence vs the number of Triangle motifs per network\")\n",
    "plt.ylim(ymin=0)\n",
    "plt.xlim(xmin=0)\n",
    "plt.savefig(\"median_time_to_90_cases_vs_triangles\",dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "plt.errorbar(Diamond_motif,median_time_to_90, yerr=yerr_tn1, markersize = 4, fmt = 'o',capsize=3,color='black',\n",
    "             ecolor='gray', elinewidth=1)\n",
    "plt.xlabel(\"Number of diamond motifs\")\n",
    "plt.ylabel('Time to ' + r'$90\\%$' + ' of total infections')\n",
    "plt.ylim(ymin=0)\n",
    "plt.xlim(xmin=0)\n",
    "#plt.title(\"Average time of peak incidence vs the number of Diamond motifs per network\")\n",
    "plt.savefig(\"median_time_to_90_cases_vs_Diamonds\",dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "plt.errorbar(Tadpole_motif,median_time_to_90, yerr=yerr_tn1, markersize = 4, fmt = 'o',capsize=3,color='black',\n",
    "             ecolor='gray', elinewidth=1)\n",
    "plt.xlabel(\"Number of tadpole motifs\")\n",
    "plt.ylabel('Time to ' + r'$90\\%$' + ' of total infections')\n",
    "plt.ylim(ymin=0)\n",
    "plt.xlim(xmin=0)\n",
    "#plt.title(\"Average time of peak incidence vs the number of Tadpole motifs per network\")\n",
    "plt.savefig(\"median_time_to_90_cases_vs_Tadpoles\",dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "plt.errorbar(Bowtie_motif,median_time_to_90, yerr=yerr_tn1, markersize = 4, fmt = 'o',capsize=3,color='black',\n",
    "             ecolor='gray', elinewidth=1)\n",
    "plt.xlabel(\"Number of bowtie motifs\")\n",
    "plt.ylabel('Time to ' + r'$90\\%$' + ' of total infections')\n",
    "plt.ylim(ymin=0)\n",
    "plt.xlim(xmin=0)\n",
    "#plt.title(\"Average time of peak incidence vs the number of Bowtie motifs per network\")\n",
    "plt.savefig(\"median_time_to_90_cases_vs_Bowties\",dpi=300, bbox_inches='tight')\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f741046",
   "metadata": {},
   "source": [
    "## Proportion of epidemics that took hold per network \"group\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1775,
   "id": "27ff3da9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ep_took_hold = [len(x) for x in Epidemics_more_than_10]\n",
    "ep_took_hold_pg = list(np.array_split(np.array(ep_took_hold), group)) # epidemics that took hold for network \"groups\" \n",
    "av_ep_took_hold_pg = [np.sum(x)/len(x) for x in y_pg] #average number of epidemics that took hold pg \n",
    "prop_ep_took_hold_pg = np.array(av_ep_took_hold_pg)/net_per_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1419,
   "id": "4fdd7fec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True, False, True, False, True)"
      ]
     },
     "execution_count": 1419,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shapiro(ep_took_hold_pg[0])[1] > 0.05, shapiro(ep_took_hold_pg[1])[1] > 0.05, shapiro(ep_took_hold_pg[2])[1] > 0.05,shapiro(ep_took_hold_pg[3])[1] > 0.05,shapiro(ep_took_hold_pg[4])[1] > 0.05,shapiro(ep_took_hold_pg[5])[1] > 0.05\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 963,
   "id": "f4672501",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = [x/no_tot_sim for x in ep_took_hold_pg] #proportion of epidemics that took hold per network\n",
    "std_pn_prop = [np.std(x) for x in ep_took_hold_pg] #std for the number of epidemics that took hold per network group\n",
    "\n",
    "five_percentile_pn= [np.percentile(x,5) for x in t] #percentile for proportion of epidemics that took hold per \"group\"\n",
    "ninty_five_percentile_pn = [np.percentile(x,95) for x in t]#percentile for proportion of epidemics that took hold per \"group\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1089,
   "id": "a9e655a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "yerr22 = [prop_ep_took_hold_pg  - np.array(five_percentile_pn), np.array(ninty_five_percentile_pn) - prop_ep_took_hold_pg]\n",
    "plt.errorbar([\"Control\", r'$\\xi = 500$' , r'$\\xi = 750$',r'$\\xi = 1000$',r'$\\xi = 1250$',r'$\\xi = 1500$'], prop_ep_took_hold_pg  , color = \"black\", yerr = np.array(yerr22)/30, markersize = 4, fmt = 'o',capsize=3,ecolor='gray', elinewidth=1)\n",
    "plt.ylim(ymin=0)\n",
    "plt.xlabel(\"Networks\")\n",
    "plt.ylabel(\"Average proportion of epidemics $>10$\")\n",
    "#plt.title(\"Average proportion of epidemics that took hold\")\n",
    "plt.savefig(\"av prop epidemics >10 with percentiles not at 0\",dpi=300, bbox_inches='tight')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1533,
   "id": "6f6a4fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "yerr22 = [prop_ep_took_hold_pg  - five_percentile_pn, ninty_five_percentile_pn - prop_ep_took_hold_pg ]\n",
    "plt.errorbar([\"Control\", r'$\\xi = 500$' , r'$\\xi = 750$',r'$\\xi = 1000$',r'$\\xi = 1250$',r'$\\xi = 1500$'], prop_ep_took_hold_pg  , color = \"black\", yerr = np.array(yerr22)/30, markersize = 4, fmt = 'o',capsize=3,ecolor='gray', elinewidth=1)\n",
    "#plt.ylim(ymin=0)\n",
    "plt.xlabel(\"Networks\")\n",
    "plt.ylabel(\"Average proportion of epidemics\")\n",
    "#plt.title(\"Average proportion of epidemics that took hold\")\n",
    "plt.savefig(\"av prop epidemics >10 with percentiles\",dpi=300, bbox_inches='tight')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34914cc",
   "metadata": {},
   "source": [
    "## Proportion of epidemics that took hold in total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1788,
   "id": "ec75bae9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion of Epidemics that took hold = 0.667037037037037\n"
     ]
    }
   ],
   "source": [
    "Epidemics_took_hold = 0\n",
    "for x in Epidemic_size:\n",
    "    if x > ex_ep:\n",
    "        Epidemics_took_hold +=1\n",
    "\n",
    "print(\"Proportion of Epidemics that took hold =\", Epidemics_took_hold/no_tot_sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b485403",
   "metadata": {},
   "source": [
    "### Proportion of epidemics took hold per network vs number of  motifs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 975,
   "id": "f6f8b4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_epidemics_took_hold_pn = np.array(ep_took_hold)/no_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2149,
   "id": "6bd0b740",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.scatter(Triangle_motif,prob_epidemics_took_hold_pn, color = \"black\")\n",
    "plt.xlabel(\"Number of triangle motifs\")\n",
    "plt.ylim(ymin=0, ymax=1)\n",
    "plt.xlim(xmin=0)\n",
    "plt.ylabel(\"Proportion of epidemics that take hold\")\n",
    "plt.title(\"Triangle motifs\")\n",
    "plt.savefig(\"Proportion_eps_took_hold_vs_triangles\", dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "plt.scatter(Diamond_motif,prob_epidemics_took_hold_pn, color = \"black\")\n",
    "plt.xlabel(\"Number of diamond motifs\")\n",
    "plt.ylim(ymin=0, ymax=1)\n",
    "plt.xlim(xmin=0)\n",
    "plt.ylabel(\"Proportion of epidemics that take hold\")\n",
    "plt.title(\"Diamond motifs\")\n",
    "plt.savefig(\"Proportion_eps_took_hold_vs_diamonds\",dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "plt.scatter(Tadpole_motif,prob_epidemics_took_hold_pn, color = \"black\")\n",
    "plt.xlabel(\"Number of tadpole motifs\")\n",
    "plt.ylim(ymin=0, ymax=1)\n",
    "plt.xlim(xmin=0)\n",
    "plt.ylabel(\"Proportion of epidemics that take hold\")\n",
    "plt.title(\"Tadpole motifs\")\n",
    "plt.savefig(\"Proportion_eps_took_hold_vs_tadpoles\",dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "\n",
    "plt.scatter(Bowtie_motif,prob_epidemics_took_hold_pn, color = \"black\")\n",
    "plt.xlabel(\"Number of bowtie motifs\")\n",
    "plt.ylim(ymin=0, ymax=1)\n",
    "plt.xlim(xmin=0)\n",
    "plt.ylabel(\"Proportion of epidemics that take hold\")\n",
    "plt.title(\"Bowtie motifs\")\n",
    "plt.savefig(\"Proportion_eps_took_hold_vs_Bowties\",dpi=300, bbox_inches='tight')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11758bcf",
   "metadata": {},
   "source": [
    "## Considering if we exploring the effect of isolates or motifs on epidemic outcomes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e25b8663",
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames4 = []\n",
    "for i in range(net):\n",
    "    name = 'Network_'+str(i)+'subgraphs.csv'\n",
    "    filenames4.append(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1733ced7",
   "metadata": {},
   "source": [
    "Maximum expected epidemic size per network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1811,
   "id": "d81b399d",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_ep_size_pn = []\n",
    "for i in range(net):\n",
    "    y = []\n",
    "    with open(filenames[i], 'r') as file:\n",
    "        data_i = file.read()\n",
    "        dat_i = np.matrix(data_i)\n",
    "        adj_i = np.reshape(dat_i, (n, n))\n",
    "        G = nx.from_numpy_matrix(adj_i)\n",
    "        t = sorted(nx.connected_components(G), key=len, reverse=True)\n",
    "        no_of_subgraphs = len(t)\n",
    "        len_of_subgraphs = [len(x) for x in t ] \n",
    "        \n",
    "        for j in range(no_of_subgraphs):\n",
    "            prop_ep_per_subgroup = (len_of_subgraphs[j]**2)/n\n",
    "            y.append(prop_ep_per_subgroup)\n",
    "        \n",
    "        max_ep_size = np.sum(y)\n",
    "        max_ep_size_pn.append(max_ep_size)\n",
    "        dat = {'Index':  list(range(no_of_subgraphs)),'Size of subgraphs': len_of_subgraphs}\n",
    "        df0 = pd.DataFrame(dat)\n",
    "        df0.to_csv('Network_'+ str(i)+ 'subgraphs.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b42ac1",
   "metadata": {},
   "source": [
    "Maximum average epidemic size excluding subgraphs of size less than or equal to 10 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1812,
   "id": "141f923d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#number_subgraphs_excl_10 = []\n",
    "size_subgraphs_excl_10 = []\n",
    "max_ep_size_excl_10_pn = []\n",
    "\n",
    "for i in range(net):\n",
    "    p = []\n",
    "    #number_subgraphs_excl_10_pn = []\n",
    "    size_subgraphs_excl_10_pn = []\n",
    "    data = pd.read_csv(filenames4[i])\n",
    "    no_sub = len(data[\"Size of subgraphs\"])\n",
    "    size_sub = data[\"Size of subgraphs\"]\n",
    "    \n",
    "    for x in size_sub:\n",
    "        if x > 10:\n",
    "            size_subgraphs_excl_10_pn.append(x)\n",
    "        \n",
    "    size_subgraphs_excl_10.append(size_subgraphs_excl_10_pn) \n",
    "    #number_subgraphs_excl_10_pn.append(len(size_subgraphs_excl_10_pn)) \n",
    "    #number_subgraphs_excl_10.append(number_subgraphs_excl_10_pn)\n",
    "    \n",
    "    for k in range(len(size_subgraphs_excl_10_pn)):\n",
    "        prop_epidemic_pn = (np.array(size_subgraphs_excl_10_pn[k]))**2/n\n",
    "        p.append(prop_epidemic_pn)\n",
    "\n",
    "    max_ep_siz = np.sum(p)\n",
    "    max_ep_size_excl_10_pn.append(max_ep_siz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1813,
   "id": "5500c1a5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "datt = {'Max epidemic size per network':  (max_ep_size_excl_10_pn),'Average epidemic size per network (excl 10)':av_ep_size_ex_10_pn}\n",
    "df00 = pd.DataFrame(datt)\n",
    "df00.to_csv('Max_ep_size_pn_vs_average_ep_size.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2005,
   "id": "0fff9303",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(Triangle_motif, np.array(av_ep_size_ex_10_pn)/np.array(max_ep_size_excl_10_pn), color = 'black')\n",
    "plt.ylim(ymin=0, ymax = 1)\n",
    "plt.xlim(xmin = 0)\n",
    "#plt.yscale(\"log\")\n",
    "plt.title('Triangle motifs')\n",
    "plt.xlabel('Number of triangle motifs')\n",
    "plt.ylabel('Normalised epidemic size')\n",
    "plt.savefig(\"max_mean_vs_average_ep_size_vs_triangles\", dpi=400, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "plt.scatter(Diamond_motif, np.array(av_ep_size_ex_10_pn)/np.array(max_ep_size_excl_10_pn), color = 'black')\n",
    "plt.ylim(ymin=0, ymax = 1)\n",
    "plt.xlim(xmin = 0)\n",
    "#plt.yscale(\"log\")\n",
    "plt.title('Diamond motifs')\n",
    "plt.xlabel('Number of diamond motifs')\n",
    "plt.ylabel('Normalised epidemic size')\n",
    "plt.savefig(\"max_mean_vs_average_ep_size_vs_diamond\", dpi=400, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "\n",
    "plt.scatter(Tadpole_motif, np.array(av_ep_size_ex_10_pn)/np.array(max_ep_size_excl_10_pn), color = 'black')\n",
    "plt.ylim(ymin=0, ymax = 1)\n",
    "plt.xlim(xmin = 0)\n",
    "#plt.yscale(\"log\")\n",
    "plt.title('Tadpole motifs')\n",
    "plt.xlabel('Number of tadpole motifs')\n",
    "plt.ylabel('Normalised epidemic size')\n",
    "plt.savefig(\"max_mean_vs_average_ep_size_vs_tadpoles\", dpi=400, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "plt.scatter(Bowtie_motif, np.array(av_ep_size_ex_10_pn)/np.array(max_ep_size_excl_10_pn), color = 'black')\n",
    "plt.ylim(ymin=0, ymax = 1)\n",
    "plt.xlim(xmin = 0)\n",
    "#plt.yscale(\"log\")\n",
    "plt.title('Bowtie motifs')\n",
    "plt.xlabel('Number of bowtie motifs')\n",
    "plt.ylabel('Normalised epidemic size')\n",
    "plt.savefig(\"max_mean_vs_average_ep_size_vs_bowties\", dpi=400, bbox_inches='tight')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2006,
   "id": "b9987e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(Triangle_motif, max_ep_size_excl_10_pn, color = 'black')\n",
    "plt.ylim(ymin = 0,ymax = 1100)\n",
    "plt.xlim(xmin = 0)\n",
    "#plt.yscale(\"log\")\n",
    "plt.xlabel(\"Number of triangle motifs\")\n",
    "plt.ylabel(\"Maximum average epidemic size\")\n",
    "#plt.title(\"Number of triangle motifs vs maximum average epidemic size\")\n",
    "plt.savefig(\"triangle_motis_vs_max_epidemic_size\",dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "plt.scatter(Bowtie_motif, max_ep_size_excl_10_pn, color = 'black')\n",
    "plt.ylim(ymin = 0,ymax = 1100)\n",
    "plt.xlim(xmin = 0)\n",
    "#plt.yscale(\"log\")\n",
    "plt.xlabel(\"Number of bowtie motifs\")\n",
    "plt.ylabel(\"Maximum average epidemic size\")\n",
    "#plt.title(\"Number of triangle motifs vs maximum average epidemic size\")\n",
    "plt.savefig(\"bowtie_motis_vs_max_epidemic_size\",dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "plt.scatter(Diamond_motif, max_ep_size_excl_10_pn, color = 'black')\n",
    "plt.ylim(ymin = 0, ymax = 1100)\n",
    "plt.xlim(xmin = 0)\n",
    "#plt.yscale(\"log\")\n",
    "plt.xlabel(\"Number of diamond motifs\")\n",
    "plt.ylabel(\"Maximum average epidemic size\")\n",
    "#plt.title(\"Number of triangle motifs vs maximum average epidemic size\")\n",
    "plt.savefig(\"diamond_motis_vs_max_epidemic_size\",dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "plt.scatter(Tadpole_motif, max_ep_size_excl_10_pn, color = 'black')\n",
    "plt.ylim(ymin = 0,ymax = 1100)\n",
    "plt.xlim(xmin = 0)\n",
    "#plt.yscale(\"log\")\n",
    "plt.xlabel(\"Number of tadpole motifs\")\n",
    "plt.ylabel(\"Maximum average epidemic size\")\n",
    "#plt.title(\"Number of triangle motifs vs maximum average epidemic size\")\n",
    "plt.savefig(\"tadpole_motis_vs_max_epidemic_size\",dpi=300, bbox_inches='tight')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1816,
   "id": "d4b11b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "av_ep_size_ex_10_pg = list(np.array_split(np.array(av_ep_size_ex_10_pn), group)) #average epidemic size per network split into per group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1817,
   "id": "f4b15d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_ep_size_excl_10_pg = list(np.array_split(np.array(max_ep_size_excl_10_pn), group)) #maximum expected ep size pg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1818,
   "id": "aeccfea7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[875.502992279502,\n",
       " 861.1394496876832,\n",
       " 837.2848990653608,\n",
       " 805.8261412743029,\n",
       " 762.8128681177398,\n",
       " 704.3716819000772]"
      ]
     },
     "execution_count": 1818,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "av_pn = [np.sum(x)/len(x) for x in av_ep_size_ex_10_pg] # calculating average epidemic size per network type (network \"group\")\n",
    "av_pn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1819,
   "id": "85a9d2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "av_std = [np.std(x,ddof = 1)  for x in av_ep_size_ex_10_pg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1820,
   "id": "918391cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#av_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1821,
   "id": "658a1cd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([14.86232974, 15.18218615, 15.03488095, 23.69753289, 27.36588698,\n",
       "       29.03099275])"
      ]
     },
     "execution_count": 1821,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q75 = [np.percentile(x, 75) for x in av_ep_size_ex_10_pg]\n",
    "q25 = [np.percentile(x, 25) for x in av_ep_size_ex_10_pg] \n",
    "np.array(q95) - np.array(q5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1822,
   "id": "f9659b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "slope, intercept, r, p, se = linregress(list(range(6)), av_pn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 986,
   "id": "2e87a9a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(677.1, 885.125, 824.464, 1000.0)"
      ]
     },
     "execution_count": 986,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(av_ep_size_ex_10_pn), max(av_ep_size_ex_10_pn), min(max_ep_size_excl_10_pn), max(max_ep_size_excl_10_pn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ad9132",
   "metadata": {},
   "source": [
    "## Comparing properties of av path length, network diameter, clustering coefficient, degree distribtuion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "ba62d50c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "av_clustering_coeff_pn = []\n",
    "global_clustering_coeff_pn = []\n",
    "for i in range(net):\n",
    "    with open(filenames[i], 'r') as file:\n",
    "        data_i = file.read()\n",
    "        dat_i = np.matrix(data_i)\n",
    "        adj_i = np.reshape(dat_i, (n, n))\n",
    "        G = nx.from_numpy_matrix(adj_i)\n",
    "        \n",
    "        average_shortest_path_l = []\n",
    "        network_diameter = []\n",
    "        p_k = []\n",
    "        \n",
    "        for C in (G.subgraph(c).copy() for c in nx.connected_components(G)):\n",
    "            av_shortest_path_l = nx.average_shortest_path_length(C)\n",
    "            net_diameter = nx.diameter(C)\n",
    "            if av_shortest_path_l != 0:\n",
    "                average_shortest_path_l.append(av_shortest_path_l)\n",
    "            if  net_diameter != 0:\n",
    "                network_diameter.append(net_diameter)\n",
    "                \n",
    "        s = adj_i.sum(axis = 1) \n",
    "        r = int(max(s))\n",
    "        for j in range(r):\n",
    "            prob = op.countOf(s,j)/n\n",
    "            p_k.append(prob)\n",
    "            \n",
    "        da = {'Average shortest path length for subgraphs': average_shortest_path_l ,'Network Diameter for subgraphs': network_diameter}\n",
    "        daa = {'P(k)': p_k, 'k': list(range(r))}\n",
    "        dfa = pd.DataFrame(da)\n",
    "        dfaa = pd.DataFrame(daa)\n",
    "        dfa.to_csv('Network_'+str(i)+ '_path_length_properties.csv')\n",
    "        dfaa.to_csv('Network_'+str(i)+ '_degree_distribution.csv')\n",
    "        \n",
    "        av_clustering_coeff = nx.average_clustering(G, nodes=None, weight=None, count_zeros=False)\n",
    "        av_clustering_coeff_pn.append(av_clustering_coeff)\n",
    "        \n",
    "        global_clustering_coeff = 3*((1/6)*np.trace(adj_i**3))/((1/2)*np.sum([int(s[i])*(int(s[i])-1) for i in range(n)]))\n",
    "        global_clustering_coeff_pn.append(global_clustering_coeff)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3995924",
   "metadata": {},
   "source": [
    "Store data for average clustering coefficients for networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "c7826e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = list(range(net))\n",
    "sample = {'Average clustering coefficient': av_clustering_coeff_pn, 'Global clustering coefficient':global_clustering_coeff_pn }\n",
    "df = pd.DataFrame(sample)\n",
    "df.to_csv('Clustering_coefficient_parameters_vs_network.csv', index = ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2016,
   "id": "79547b44",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sample = pd.read_csv('Clustering_coefficient_parameters_vs_network.csv')\n",
    "\n",
    "plt.scatter(Triangle_motif, sample[\"Average clustering coefficient\"], color = \"black\")\n",
    "#plt.yscale(\"log\")\n",
    "plt.ylim(ymin = 0, ymax = 0.4)\n",
    "plt.xlim(xmin = 0)\n",
    "plt.xlabel(\"Number of triangle motifs\")\n",
    "plt.ylabel(\"Average clustering coefficient\")\n",
    "\n",
    "#plt.title(\"Average clustering coefficient excluding isolates per network\")\n",
    "plt.savefig(\"Average clustering coefficient excluding isolates per network\",dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "plt.scatter(Triangle_motif, sample[\"Global clustering coefficient\"], color = \"black\")\n",
    "#plt.yscale(\"log\")\n",
    "plt.ylim(ymin = 0, ymax = 0.13)\n",
    "plt.xlim(xmin = 0)\n",
    "plt.xlabel(\"Number of triangle motifs\")\n",
    "plt.ylabel(\"Global clustering coefficient\")\n",
    "#plt.title(\"Global clustering coefficient per network\")\n",
    "plt.savefig(\"Global clustering coefficient per network\",dpi=300, bbox_inches='tight')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "id": "b90d5118",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.1204945987609015, 0.1605810438799894)"
      ]
     },
     "execution_count": 575,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample[\"Average clustering coefficient\"][0], sample[\"Average clustering coefficient\"][29]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "id": "fd1537fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "av_clust_coeff_pg = list(np.array_split(np.array(sample[\"Average clustering coefficient\"]), group))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "id": "22f44653",
   "metadata": {},
   "outputs": [],
   "source": [
    "glob_clust_coeff_pg =  list(np.array_split(np.array(sample[\"Global clustering coefficient\"]), group))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2087,
   "id": "eb61b6eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.01665532781358701,\n",
       " 0.034428726546803796,\n",
       " 0.05193785205947775,\n",
       " 0.06469297227081448,\n",
       " 0.08470635377052352,\n",
       " 0.10126781156959058]"
      ]
     },
     "execution_count": 2087,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_glob = [np.mean(x) for x in glob_clust_coeff_pg]\n",
    "mean_glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2086,
   "id": "5af7c6da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.14768060931883617,\n",
       " 0.16256970155765998,\n",
       " 0.18837844168574946,\n",
       " 0.22234849494795156,\n",
       " 0.26175723940508494,\n",
       " 0.29486683521085727]"
      ]
     },
     "execution_count": 2086,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_av = [np.mean(x) for x in av_clust_coeff_pg]\n",
    "mean_av"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29999c97",
   "metadata": {},
   "source": [
    "## Exploring path length and network diameter properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "ce454ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames7 = []\n",
    "for i in range(net):\n",
    "    name = 'Network_'+str(i)+ '_path_length_properties.csv'\n",
    "    filenames7.append(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1907,
   "id": "0ca84b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_path_subgraph_length = []\n",
    "max_subgraph_diameter = []\n",
    "\n",
    "for i in range(net):\n",
    "    data = pd.read_csv(filenames7[i])\n",
    "    av_short_path_l = data['Average shortest path length for subgraphs']\n",
    "    network_diam = data['Network Diameter for subgraphs']\n",
    "    max_av_short_path_l = max(av_short_path_l)\n",
    "    max_path_subgraph_length.append(max_av_short_path_l)\n",
    "    network_diam_max = max(network_diam)\n",
    "    max_subgraph_diameter.append(network_diam_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1908,
   "id": "1e7e0b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_subgraph_diameter_pg = list(np.array_split(np.array(max_subgraph_diameter), group))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2019,
   "id": "69cf1940",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(Triangle_motif, max_subgraph_diameter, color = \"black\")\n",
    "plt.xlabel(\"Number of triangle motifs\")\n",
    "plt.ylabel(\"Maximum sub-graph diameter\")\n",
    "plt.ylim(ymin = 0, ymax = 15)\n",
    "plt.xlim(xmin = 0)\n",
    "#plt.title(\"Maximum subgraph diameter vs number of triangle motifs per network\")\n",
    "plt.savefig(\"Maximum subgraph diameter per network vs number of triangle motifs per network\",dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "plt.scatter(Triangle_motif,  max_path_subgraph_length, color = \"black\")\n",
    "plt.xlabel(\"Number of triangle motifs\")\n",
    "plt.ylim(ymin = 0, ymax = 6)\n",
    "plt.xlim(xmin = 0)\n",
    "plt.ylabel(\"Maximum average sub-graph path length\")\n",
    "#plt.title(\"Maximum average sub-graph path length vs number of triangle motifs per network\")\n",
    "plt.savefig(\"Maximum average subgraph path length vs number of triangle motifs per network\",dpi=300, bbox_inches='tight')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93671b22",
   "metadata": {},
   "source": [
    "## Exploring degree distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "d1aaa9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames5 = []\n",
    "for i in range(net):\n",
    "    name = 'Network_'+str(i)+ '_degree_distribution.csv'\n",
    "    filenames5.append(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4657737e",
   "metadata": {},
   "source": [
    "Calculating the maximum degree for each network, to find the maximum degree for 180 networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a0c2807b",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_degree_k = []\n",
    "for i in range(net):\n",
    "    data = pd.read_csv(filenames5[i])\n",
    "    degree_k = list(data[\"k\"])\n",
    "    max_degree = degree_k[-1]\n",
    "    max_degree_k.append(max_degree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1495,
   "id": "1773bde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_degree_k_pg = list(np.array_split(np.array(max_degree_k), group))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1528,
   "id": "10adf7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "av_deg_max_pg = [np.sum(x)/net_per_group for x in max_degree_k_pg] \n",
    "#av_deg_max_pg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841385a4",
   "metadata": {},
   "source": [
    "Create a list for p(k) of each network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "5b4cbb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_k =[]\n",
    "for i in range(net):\n",
    "    data = pd.read_csv(filenames5[i])\n",
    "    p_kk = list(data[\"P(k)\"])\n",
    "    \n",
    "    if len(p_kk) < (max(max_degree_k)+1):\n",
    "        t = (max(max_degree_k)+1)-len(p_kk)\n",
    "        for j in range(t):\n",
    "            p_kk.append(0)\n",
    "    \n",
    "    p_k.append(p_kk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "3b89cba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_k_pn = list(np.array_split(np.array(p_k), net))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1e9416",
   "metadata": {},
   "source": [
    "Calculating average p(k) per group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a0803b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_k_pg = list(np.array_split(np.array(p_k), group))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "dd8acb39",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "av_p_k_g1 = np.zeros(max(max_degree_k)+1)\n",
    "av_p_k_g2 = np.zeros(max(max_degree_k)+1)\n",
    "av_p_k_g3 = np.zeros(max(max_degree_k)+1)\n",
    "av_p_k_g4 = np.zeros(max(max_degree_k)+1)\n",
    "av_p_k_g5 = np.zeros(max(max_degree_k)+1)\n",
    "av_p_k_g6 = np.zeros(max(max_degree_k)+1)\n",
    "\n",
    "for i in range(30):\n",
    "    av_p_k_g1 = av_p_k_g1 +  p_k_pn[i]\n",
    "for j in range(30,60):\n",
    "    av_p_k_g2 = av_p_k_g2 +  p_k_pn[j]\n",
    "for k in range(60,90):\n",
    "    av_p_k_g3 = av_p_k_g3 +  p_k_pn[k]\n",
    "for l in range(90,120):\n",
    "    av_p_k_g4 = av_p_k_g4 +  p_k_pn[l]\n",
    "for p in range(120,150):\n",
    "    av_p_k_g5 = av_p_k_g5 +  p_k_pn[p]\n",
    "for t in range(150,180):\n",
    "    av_p_k_g6 = av_p_k_g6 +  p_k_pn[t]\n",
    "    \n",
    "average_pk_g1 = av_p_k_g1/net_per_group\n",
    "average_pk_g2 = av_p_k_g2/net_per_group\n",
    "average_pk_g3 = av_p_k_g3/net_per_group\n",
    "average_pk_g4 = av_p_k_g4/net_per_group\n",
    "average_pk_g5 = av_p_k_g5/net_per_group\n",
    "average_pk_g6 = av_p_k_g6/net_per_group"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa500009",
   "metadata": {},
   "source": [
    "Plotting degree distribution for control networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2062,
   "id": "fe7d2f4b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "k = max(max_degree_k)+1\n",
    "kk = list(range(k))\n",
    "degree= list(range(1,k))\n",
    "plt.scatter(kk,average_pk_g1, color = \"black\")\n",
    "plt.plot(degree, [(1/(x**(2))) for x in degree], '--r', label = r'$\\gamma = 2$')\n",
    "plt.plot(degree, [(1/(x**(3))) for x in degree], '--b', label = r'$\\gamma = 3$')\n",
    "plt.xlabel(\"k\")\n",
    "plt.legend()\n",
    "plt.ylabel(\"p(k)\")\n",
    "plt.xscale(\"log\")\n",
    "plt.yscale(\"log\")\n",
    "plt.title(\"Control group\")\n",
    "plt.savefig(\"degree_distribution_barabasi\",dpi=300, bbox_inches='tight')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d68716",
   "metadata": {},
   "source": [
    "Ploting average for all rewired networks: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "73b4bed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "av_pk_all_net = (av_p_k_g2 +av_p_k_g3 +av_p_k_g4 +av_p_k_g5 + av_p_k_g6)/(net-net_per_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2063,
   "id": "1a991e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(kk,av_pk_all_net, color = \"black\")\n",
    "plt.plot(degree, [(1/(x)**(2)) for x in degree ], '--r', label = r'$\\gamma = 2$')\n",
    "plt.plot(degree, [(1/(x)**(3)) for x in degree ], '--b', label = r'$\\gamma = 3$')\n",
    "plt.xlabel(\"k\")\n",
    "plt.legend()\n",
    "plt.ylabel(\"p(k)\")\n",
    "plt.xscale(\"log\")\n",
    "plt.yscale(\"log\")\n",
    "plt.title(\"Rewired networks\")\n",
    "plt.savefig(\"degree_distribution_rewired_networks\",dpi=300, bbox_inches='tight')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "888ea018",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\"Average p(k) group 1\":average_pk_g1[0],\n",
    "       \"Average p(k) group 2\":average_pk_g2[0],\n",
    "       \"Average p(k) group 3\":average_pk_g3[0],\n",
    "       \"Average p(k) group 4\":average_pk_g4[0],\n",
    "       \"Average p(k) group 5\":average_pk_g5[0],\n",
    "       \"Average p(k) group 6\":average_pk_g6[0],\n",
    "       \"Average p(k) all networks\": av_pk_all_net[0]}\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv('Average_degree_distribution_pg.csv', index = kk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ea5b00",
   "metadata": {},
   "source": [
    "# Number of subgraphs per network and average per group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "91d77dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames4 = []\n",
    "for i in range(net):\n",
    "    name = 'Network_'+str(i)+'subgraphs.csv'\n",
    "    filenames4.append(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1807,
   "id": "f430b4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_subgraphs_pn = []\n",
    "for i in range(net):\n",
    "    data = pd.read_csv(filenames4[i])\n",
    "    no_sub = len(data[\"Number of Subgraphs\"])\n",
    "    number_of_subgraphs_pn.append(no_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1808,
   "id": "8b8afa83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(156, 83)"
      ]
     },
     "execution_count": 1808,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_of_subgraphs_pn.index(max(number_of_subgraphs_pn)), max(number_of_subgraphs_pn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2013,
   "id": "8e2d55fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(Triangle_motif, number_of_subgraphs_pn, color = \"black\")\n",
    "plt.title(\"Triangle motif\")\n",
    "plt.xlabel(\"Number of triangle motifs\")\n",
    "plt.ylabel(\"Number of sub-graphs\")\n",
    "plt.ylim(ymin = 0)\n",
    "plt.xlim(xmin = 0)\n",
    "plt.savefig(\"no_subgraphs_vs_triangle\",dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "plt.scatter(Diamond_motif, number_of_subgraphs_pn, color = \"black\")\n",
    "plt.title(\"Diamond motif\")\n",
    "plt.xlabel(\"Number of diamond motifs\")\n",
    "plt.ylabel(\"Number of sub-graphs\")\n",
    "plt.ylim(ymin = 0)\n",
    "plt.xlim(xmin = 0)\n",
    "plt.savefig(\"no_subgraphs_vs_diamond\",dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "plt.scatter(Tadpole_motif, number_of_subgraphs_pn, color = \"black\")\n",
    "plt.title(\"Tadpole motif\")\n",
    "plt.xlabel(\"Number of tadpole motifs\")\n",
    "plt.ylabel(\"Number of sub-graphs\")\n",
    "plt.ylim(ymin = 0)\n",
    "plt.xlim(xmin = 0)\n",
    "plt.savefig(\"no_subgraphs_vs_tadpole\",dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "plt.scatter(Bowtie_motif, number_of_subgraphs_pn, color = \"black\")\n",
    "plt.title(\"Bowtie motif\")\n",
    "plt.xlabel(\"Number of bowtie motifs\")\n",
    "plt.ylabel(\"Number of sub-graphs\")\n",
    "plt.ylim(ymin = 0)\n",
    "plt.xlim(xmin = 0)\n",
    "plt.savefig(\"no_subgraphs_vs_bowtie\",dpi=300, bbox_inches='tight')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160ec45f",
   "metadata": {},
   "source": [
    "Average number of subgraphs per group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1803,
   "id": "013e2493",
   "metadata": {},
   "outputs": [],
   "source": [
    "av_subgraph_pg = []\n",
    "sub_percentile_95_pg = []\n",
    "sub_percentile_5_pg = []\n",
    "subgraph_variance_pg =[]\n",
    "subgraph_std_pg = []\n",
    "number_of_subgraphs_pg = list(np.array_split(np.array(number_of_subgraphs_pn), group)) \n",
    "for i in range(group):\n",
    "    av_sub = (np.sum(number_of_subgraphs_pg[i]))/net_per_group\n",
    "    av_subgraph_pg.append(av_sub)\n",
    "    std = np.std(number_of_subgraphs_pg[i])\n",
    "    var = np.var(number_of_subgraphs_pg[i])\n",
    "    per_95 = np.percentile(number_of_subgraphs_pg[i],95)\n",
    "    per_5 = np.percentile(number_of_subgraphs_pg[i],5)\n",
    "    subgraph_std_pg.append(std)\n",
    "    subgraph_variance_pg.append(var)\n",
    "    sub_percentile_95_pg.append(per_95)\n",
    "    sub_percentile_5_pg.append(per_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2105,
   "id": "77a91a0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0,\n",
       " 1.4,\n",
       " 4.733333333333333,\n",
       " 13.233333333333333,\n",
       " 32.266666666666666,\n",
       " 68.76666666666667]"
      ]
     },
     "execution_count": 2105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "av_subgraph_pg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2118,
   "id": "e5a8fa2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "size_of_subgraphs_pn_ = []\n",
    "for i in range(net):\n",
    "    size_of_subgraphs_pn = []\n",
    "    data = pd.read_csv(filenames4[i])\n",
    "    size_sub = np.array(data[\"Size of subgraphs\"])\n",
    "    for j in range(len(size_sub)):\n",
    "        sub_size = size_sub[j]\n",
    "        size_of_subgraphs_pn.append(sub_size)\n",
    "    size_of_subgraphs_pn_.append(size_of_subgraphs_pn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2125,
   "id": "a6677e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_size_of_subgraphs_pg = [max(i) for i in size_of_subgraphs_pn_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2128,
   "id": "6d5bcc2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_size_of_subgraphs_pg = list(np.array_split(np.array(max_size_of_subgraphs_pg), group)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2130,
   "id": "49d63b9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1000.0,\n",
       " 999.6,\n",
       " 996.2333333333333,\n",
       " 987.3666666666667,\n",
       " 967.1666666666666,\n",
       " 926.4666666666667]"
      ]
     },
     "execution_count": 2130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "av_max_size_of_subgraphs_pg = [np.mean(i) for i in max_size_of_subgraphs_pg ]\n",
    "av_max_size_of_subgraphs_pg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1902,
   "id": "43fdd7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "size_of_subgraphs_pg\n",
    "size_of_subgraphs_pg_ex_1_pg =[]\n",
    "for x in size_of_subgraphs_pg:\n",
    "    size_of_subgraphs_pg_ex_1 =[]\n",
    "    for j in x:\n",
    "        if j>3:\n",
    "            size_of_subgraphs_pg_ex_1.append(j)\n",
    "    size_of_subgraphs_pg_ex_1_pg.append(size_of_subgraphs_pg_ex_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336c9511",
   "metadata": {},
   "source": [
    "# Number of isolated nodes per network and average per group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1895,
   "id": "39225423",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_isolated_nodes_pn = []\n",
    "for i in range(net):\n",
    "    count = 0\n",
    "    data = pd.read_csv(filenames4[i])\n",
    "    l_sub = data[\"Size of subgraphs\"]\n",
    "    for i in range(len(l_sub)):\n",
    "        if l_sub[i] == 1:\n",
    "            count +=1\n",
    "    number_of_isolated_nodes_pn.append(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1896,
   "id": "bed03192",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(156, 77)"
      ]
     },
     "execution_count": 1896,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_of_isolated_nodes_pn.index(max(number_of_isolated_nodes_pn)),max(number_of_isolated_nodes_pn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1897,
   "id": "0d0e0f83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0, 0.4, 3.7, 11.866666666666667, 29.766666666666666, 62.8]"
      ]
     },
     "execution_count": 1897,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_of_isolated_nodes_pg = list(np.array_split(np.array(number_of_isolated_nodes_pn), group))\n",
    "av_per_number_of_isolated_nodes_pg = [np.sum(x)/net_per_group for x in number_of_isolated_nodes_pg]\n",
    "av_per_number_of_isolated_nodes_pg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2014,
   "id": "893911c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(Triangle_motif, number_of_isolated_nodes_pn, color = \"black\")\n",
    "plt.title(\"Triangle motif\")\n",
    "plt.xlabel(\"Number of triangle motifs\")\n",
    "plt.ylabel(\"Number of isolated nodes\")\n",
    "plt.ylim(ymin = 0)\n",
    "plt.xlim(xmin = 0)\n",
    "plt.savefig(\"no_isolated_nodes_triangle\",dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "plt.scatter(Diamond_motif, number_of_isolated_nodes_pn, color = \"black\")\n",
    "plt.title(\"Diamond motif\")\n",
    "plt.xlabel(\"Number of diamond motifs\")\n",
    "plt.ylabel(\"Number of isolated nodes\")\n",
    "plt.ylim(ymin = 0)\n",
    "plt.xlim(xmin = 0)\n",
    "plt.savefig(\"no_isolated_nodes_diamond\",dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "plt.scatter(Tadpole_motif, number_of_isolated_nodes_pn, color = \"black\")\n",
    "plt.title(\"Tadpole motif\")\n",
    "plt.xlabel(\"Number of tadpole motifs\")\n",
    "plt.ylabel(\"Number of isolated nodes\")\n",
    "plt.ylim(ymin = 0)\n",
    "plt.xlim(xmin = 0)\n",
    "plt.savefig(\"no_isolated_nodes_tadpole\",dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "plt.scatter(Bowtie_motif, number_of_isolated_nodes_pn, color = \"black\")\n",
    "plt.title(\"Bowtie motif\")\n",
    "plt.xlabel(\"Number of bowtie motifs\")\n",
    "plt.ylabel(\"Number of isolated nodes\")\n",
    "plt.ylim(ymin = 0)\n",
    "plt.xlim(xmin = 0)\n",
    "plt.savefig(\"no_isolated_nodes_bowtie\",dpi=300, bbox_inches='tight')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d229eb",
   "metadata": {},
   "source": [
    "## Isolates and subgraphs compared with triangle motifs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1100,
   "id": "03fa3b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(Triangle_motif,number_of_isolated_nodes_pn, color = \"black\")\n",
    "plt.xlabel(\"Number of triangle motifs\")\n",
    "plt.ylabel(\"Number of isolated nodes\")\n",
    "plt.title(\"The number of triangle motifs vs number of isolated nodes per network\")\n",
    "plt.savefig(\"number_of_triangles_vs_isolated_nodes_pn\",dpi=300, bbox_inches='tight')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1893,
   "id": "82765f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(Triangle_motif,number_of_subgraphs_pn, color = \"black\")\n",
    "plt.xlabel(\"Number of triangle motifs\")\n",
    "plt.ylabel(\"Number of subgraphs\")\n",
    "plt.title(\"The number of triangle motifs vs number of subgraphs per network\")\n",
    "plt.savefig(\"no_triangles_vs_no_subgraphs_pn\",dpi=300, bbox_inches='tight')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925fd94f",
   "metadata": {},
   "source": [
    "## SUB ANALYSIS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ebecbf",
   "metadata": {},
   "source": [
    "Probability of epidemics of size 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "c57974f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.441311\n"
     ]
    }
   ],
   "source": [
    "num = 10**6\n",
    "recover = expon.rvs(scale=gamma, size=num)\n",
    "transmit = expon.rvs(scale=tau, size=num)\n",
    "count = 0\n",
    "\n",
    "for i in range(num):\n",
    "    if recover[i]<transmit[i]:\n",
    "        count +=1\n",
    "\n",
    "epidemics_prob = count/num\n",
    "print(epidemics_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b486b5e",
   "metadata": {},
   "source": [
    "### Proportion of epicemics with expected epidemic size 1:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b341d87",
   "metadata": {},
   "source": [
    "### Using networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1293,
   "id": "45da752b",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_neighbour = []\n",
    "one_neighbour = []\n",
    "two_neighbour = []\n",
    "three_neighbour = []\n",
    "four_neighbour = []\n",
    "five_neighbour = []\n",
    "six_neighbour = []\n",
    "seven_neighbour = []\n",
    "eight_neighbour = []\n",
    "nine_neighbour = []\n",
    "ten_neighbour = []\n",
    "\n",
    "\n",
    "\n",
    "for i in range(net):\n",
    "    with open(filenames[i], 'r') as file:\n",
    "        data_i = file.read()\n",
    "        dat_i = np.matrix(data_i)\n",
    "        adj_i = np.reshape(dat_i, (n, n))\n",
    "        G = nx.from_numpy_matrix(adj_i)\n",
    "        \n",
    "        s = adj_i.sum(axis = 1)\n",
    "        \n",
    "        no_neigh = op.countOf(s, 0)\n",
    "        one_neigh = op.countOf(s, 1)\n",
    "        two_neigh = op.countOf(s, 2)\n",
    "        three_neigh = op.countOf(s, 3)\n",
    "        four_neigh = op.countOf(s, 4)\n",
    "        five_neigh = op.countOf(s, 5)\n",
    "        six_neigh = op.countOf(s, 6)\n",
    "        seven_neigh = op.countOf(s, 7)\n",
    "        eight_neigh = op.countOf(s, 8)\n",
    "        nine_neigh = op.countOf(s, 9)\n",
    "        ten_neigh = op.countOf(s,10)\n",
    "   \n",
    "        one_neighbour.append(one_neigh)\n",
    "        two_neighbour.append(two_neigh)\n",
    "        three_neighbour.append(three_neigh)\n",
    "        four_neighbour.append(four_neigh)\n",
    "        five_neighbour.append(five_neigh)\n",
    "        six_neighbour.append(six_neigh)\n",
    "        seven_neighbour.append(seven_neigh)\n",
    "        eight_neighbour.append(eight_neigh)\n",
    "        nine_neighbour.append(nine_neigh)\n",
    "        ten_neighbour.append(ten_neigh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1536,
   "id": "ab373b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "prop_no = (np.sum(np.array(no_neighbour))/net)/n #net = 180 networks, n = 1000 nodes in each network\n",
    "prop_one = (np.sum(np.array(one_neighbour))/net)/n\n",
    "prop_two = (np.sum(np.array(two_neighbour))/net)/n\n",
    "prop_three = (np.sum(np.array(three_neighbour))/net)/n\n",
    "prop_four = (np.sum(np.array(four_neighbour))/net)/n\n",
    "prop_five = (np.sum(np.array(five_neighbour))/net)/n\n",
    "prop_six = (np.sum(np.array(six_neighbour))/net)/n\n",
    "prop_seven = (np.sum(np.array(seven_neighbour))/net)/n\n",
    "prop_eight = (np.sum(np.array(eight_neighbour))/net)/n\n",
    "prop_nine = (np.sum(np.array(nine_neighbour))/net)/n\n",
    "prop_ten = (np.sum(np.array(ten_neighbour))/net)/n\n",
    "\n",
    "#m = tau/(tau+gamma) #p non-transmit \n",
    "m = gamma/(tau+gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec71483",
   "metadata": {},
   "source": [
    "### Using degree distribution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51314971",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_k_0 = 0\n",
    "p_k_1 = 0\n",
    "p_k_2 = 0\n",
    "p_k_3 = 0\n",
    "p_k_4 = 0\n",
    "p_k_5 = 0\n",
    "p_k_6 = 0\n",
    "p_k_7 = 0\n",
    "p_k_8 = 0\n",
    "p_k_9 = 0\n",
    "p_k_10 = 0\n",
    "for j in p_k_pn:\n",
    "    for i in j:\n",
    "        p_k_0 = p_k_0+i[0]\n",
    "        p_k_1 = p_k_1+i[1]\n",
    "        p_k_2 = p_k_2+i[2]\n",
    "        p_k_3 = p_k_3+i[3]\n",
    "        p_k_4 = p_k_4+i[4]\n",
    "        p_k_5 = p_k_5+i[5]\n",
    "        p_k_6 = p_k_6+i[6]\n",
    "        p_k_7 = p_k_7+i[7]\n",
    "        p_k_8 = p_k_8+i[8]\n",
    "        p_k_9 = p_k_9+i[9]\n",
    "        p_k_10 = p_k_10+i[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1843,
   "id": "a2f6d7d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.018088888888888894,\n",
       " 0.058727777777777775,\n",
       " 0.11656666666666668,\n",
       " 0.2285000000000001,\n",
       " 0.15421111111111113,\n",
       " 0.10143888888888895,\n",
       " 0.07068333333333335,\n",
       " 0.04946111111111108,\n",
       " 0.03752222222222221,\n",
       " 0.029399999999999996,\n",
       " 0.022805555555555534)"
      ]
     },
     "execution_count": 1843,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_k_0/net , p_k_1/net, p_k_2/net , p_k_3/net ,p_k_4/net ,p_k_5/net ,p_k_6/net ,p_k_7/net , p_k_8/net , p_k_9/net , p_k_10/net "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2089,
   "id": "f1634efb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected proportion of epidemics of size 1 =  0.1513183806916357\n",
      "Expected proportion of epidemics of size 1, using degree distribution =  0.15132726958052461\n",
      "Proportion of epidemics of size 1 =  0.26222222222222225\n"
     ]
    }
   ],
   "source": [
    "prop_ep_size_one = (0.01808 + prop_one*(m) + prop_two*(m**2) + prop_three*(m**3)+ prop_four*(m**4)+ prop_five*(m**5)+ prop_six*(m**6)+ prop_seven*(m**7)+ prop_eight*(m**8)+ prop_nine*(m**9) + prop_ten*(m**10))\n",
    "prop_ep_size_one_2 = ((p_k_0/net) + (p_k_1/net)*(m) + (p_k_2/net)*(m**2) + (p_k_3/net)*(m**3)+ (p_k_4/net)*(m**4)+ (p_k_5/net)*(m**5)+ (p_k_6/net)*(m**6)+ (p_k_7/net)*(m**7)+ (p_k_8/net)*(m**8)+ (p_k_9/net)*(m**9) + (p_k_10/net)*(m**10))\n",
    "print(\"Expected proportion of epidemics of size 1 = \", prop_ep_size_one)\n",
    "print(\"Expected proportion of epidemics of size 1, using degree distribution = \", prop_ep_size_one_2)\n",
    "print(\"Proportion of epidemics of size 1 = \", (op.countOf(Epidemic_size,1)/no_tot_sim))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2090,
   "id": "b4ab8fac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0,\n",
       " 0.058727777777777775,\n",
       " 0.11656666666666667,\n",
       " 0.2285,\n",
       " 0.1542111111111111,\n",
       " 0.10143888888888888,\n",
       " 0.07068333333333333,\n",
       " 0.04946111111111111,\n",
       " 0.03752222222222223,\n",
       " 0.0294,\n",
       " 0.022805555555555558)"
      ]
     },
     "execution_count": 2090,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prop_no,prop_one,prop_two,prop_three,prop_four,prop_five,prop_six,prop_seven,prop_eight,prop_nine,prop_ten "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9b66cd",
   "metadata": {},
   "source": [
    "Calculating expected R_0 for networks: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1481,
   "id": "fa4f5a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "R_0_pn = []\n",
    "for i in range(net):\n",
    "    with open(filenames[i], 'r') as file:\n",
    "        data_i = file.read()\n",
    "        dat_i = np.matrix(data_i)\n",
    "        adj_i = np.reshape(dat_i, (n, n))\n",
    "        \n",
    "        G = nx.from_numpy_matrix(adj_i)\n",
    "        R_0 = EoN.estimate_R0(G, tau = 0.06, gamma = 0.076, transmissibility = 0.06/(0.06+0.076))\n",
    "        R_0_pn.append(R_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1482,
   "id": "5766450d",
   "metadata": {},
   "outputs": [],
   "source": [
    "R_0_pg = list(np.array_split(np.array(R_0_pn), group))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1915,
   "id": "f86cd3e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6.8 , 6.82, 6.88, 7.54, 7.46, 7.9 ])"
      ]
     },
     "execution_count": 1915,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(np.array(av_R_0_pg),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1563,
   "id": "9136f08c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5567409360148025"
      ]
     },
     "execution_count": 1563,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "av_R_0_pg = [np.mean(x) for x in R_0_pg]\n",
    "av_R_0_pg\n",
    "np.sum(np.array(av_R_0_pg)*0.077)/6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1562,
   "id": "df410c07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.230401766426006"
      ]
     },
     "execution_count": 1562,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(av_R_0_pg)/6"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
